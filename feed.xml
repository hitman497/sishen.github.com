<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Do Great Things - 叶玎玎</title>
  <subtitle> 叶玎玎在技术、创业、团队协作和项目管理上的思考和分享</subtitle>
  <id>http://yedingding.com</id>
  <link href="http://yedingding.com/"/>
  <link href="http://yedingding.com/feed.xml" rel="self"/>
  <updated>2014-07-09T12:00:00+08:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>Deliver Better Product (I)</title>
    <link rel="alternate" href="/2014/07/09/deliver-better-product-i.html"/>
    <id>/2014/07/09/deliver-better-product-i.html</id>
    <published>2014-07-09T12:00:00+08:00</published>
    <updated>2014-07-09T12:00:00+08:00</updated>
    <author>
      <name>叶玎玎</name>
    </author>
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Most Products Fail!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;是的，大多数的产品都会死掉，一个黑暗的事实。就如很多人认为现在团队协作工具出来这么多，很不看好风车一样。但是就如风车诞生的初衷一样，我们希望风车能够帮助创业团队更好的成长，更快地发布更好的产品，很欣慰现在风车真的帮助到了不少产品团队，让我能更有动力去改进产品和提供更好的服务。之所以我相信风车能真正帮到用户发布更好的产品，是因为我坚信对于一个创业团队来说，采用正确的做事方式和合适的工具能大大降低失败几率。&lt;/p&gt;

&lt;p&gt;一个产品的成功，也许需要天时地利人和&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;，但是要做到不失败相对就容易的多了。我想分享一些我们做事的方式，希望能帮助到你。这里没有互联网思维，这里没有成功学，有的只是真正的工作实践心得，甚至有些也许都是很笨的，但是希望这些分享能给你带来一点点的启发，并且还能付诸于实践。&lt;/p&gt;

&lt;p&gt;&lt;aside class="“aside”"&gt;
  &lt;img alt="Scrum Roles" src="/images/deliver-better-product-i/KnibergRoles.jpg?1404985052"&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;上图是对 Scrum 敏捷方法不同角色的职责的很好诠释，即使是对于不采用 Scrum 的团队来说，我们也应该这样去做产品。做正确的事情，正确的做事情，并且快速的做事情，这样一个团队，最佳情况就是总是能在正确的时间用正确的方法做正确的事，非常完美。对于这个系列的计划，我希望能涵盖产品的整个生命线。本文开篇，主要介绍如何做产品远景和形态探索，后面的文章会涉及如何做用户角色分析，如何做用户故事、如何做计划评估、如果做迭代计划、如何协作执行、如何做回顾测试、如何做用户访谈等&lt;sup id="fnref2"&gt;&lt;a href="#fn2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Great Product starts with Vision&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;正如我在&lt;a href="http://yedingding.com/2014/06/17/know-your-consumers.html"&gt;「你是否关注过消费者心理？」&lt;/a&gt; 所写，优秀的市场营销者，会先去传递产品的使命，然后才是与使命匹配的具体需求实现。对外如此，对内同样需要如此，不然团队事情会做的很茫然。为什么我们要做这个功能，为什么我们不做那个功能？为什么我们现在需要做这个，而不是那个？回忆一下，曾经你有没有问过自己这些问题，为什么会...&lt;/p&gt;</summary>
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;Most Products Fail!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;是的，大多数的产品都会死掉，一个黑暗的事实。就如很多人认为现在团队协作工具出来这么多，很不看好风车一样。但是就如风车诞生的初衷一样，我们希望风车能够帮助创业团队更好的成长，更快地发布更好的产品，很欣慰现在风车真的帮助到了不少产品团队，让我能更有动力去改进产品和提供更好的服务。之所以我相信风车能真正帮到用户发布更好的产品，是因为我坚信对于一个创业团队来说，采用正确的做事方式和合适的工具能大大降低失败几率。&lt;/p&gt;

&lt;p&gt;一个产品的成功，也许需要天时地利人和&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;，但是要做到不失败相对就容易的多了。我想分享一些我们做事的方式，希望能帮助到你。这里没有互联网思维，这里没有成功学，有的只是真正的工作实践心得，甚至有些也许都是很笨的，但是希望这些分享能给你带来一点点的启发，并且还能付诸于实践。&lt;/p&gt;

&lt;p&gt;&lt;aside class=“aside”&gt;
  &lt;img alt="Scrum Roles" src="http://yedingding.com/images/deliver-better-product-i/KnibergRoles.jpg?1404985052" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;上图是对 Scrum 敏捷方法不同角色的职责的很好诠释，即使是对于不采用 Scrum 的团队来说，我们也应该这样去做产品。做正确的事情，正确的做事情，并且快速的做事情，这样一个团队，最佳情况就是总是能在正确的时间用正确的方法做正确的事，非常完美。对于这个系列的计划，我希望能涵盖产品的整个生命线。本文开篇，主要介绍如何做产品远景和形态探索，后面的文章会涉及如何做用户角色分析，如何做用户故事、如何做计划评估、如果做迭代计划、如何协作执行、如何做回顾测试、如何做用户访谈等&lt;sup id="fnref2"&gt;&lt;a href="#fn2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Great Product starts with Vision&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;正如我在&lt;a href="http://yedingding.com/2014/06/17/know-your-consumers.html"&gt;「你是否关注过消费者心理？」&lt;/a&gt; 所写，优秀的市场营销者，会先去传递产品的使命，然后才是与使命匹配的具体需求实现。对外如此，对内同样需要如此，不然团队事情会做的很茫然。为什么我们要做这个功能，为什么我们不做那个功能？为什么我们现在需要做这个，而不是那个？回忆一下，曾经你有没有问过自己这些问题，为什么会问这些问题。究其原因，其实是产品的使命和远景缺乏透明，或者不够简单和清晰，最终导致了执行层面的混乱。用句最近流行的话来讲，『你不要用战术的勤奋来掩盖战略的懒惰』，不要去逃避寻找远景这个答案。只有有了产品远景，后面的路才能走得顺，不然相信我，出来混，迟早是要还的。&lt;/p&gt;

&lt;p&gt;&lt;aside class=“aside”&gt;
  &lt;img alt="Vision" src="http://yedingding.com/images/deliver-better-product-i/vision.jpg?1404985052" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;那么，应该如何才能定出一个好的产品远景呢？作为一个创业者，我们时常被要求用一句话来描述清楚产品，远景也应该如此。经典的 30 秒电梯游说就非常适合用来做产品远景测试，&lt;/p&gt;

&lt;blockquote&gt;
&lt;b&gt;For&lt;/b&gt; (target customer)&lt;br/&gt;
&lt;b&gt;Who&lt;/b&gt; (statement of the need or opportunity)&lt;br/&gt;
&lt;b&gt;The&lt;/b&gt; (product name) &lt;b&gt;is a&lt;/b&gt; (product category)&lt;br/&gt;
&lt;b&gt;That&lt;/b&gt; (key benefit, compelling reason to buy)&lt;br/&gt;
&lt;b&gt;Unlike&lt;/b&gt; (primary competitive alternative)&lt;br/&gt;
&lt;b&gt;Our product&lt;/b&gt; (statement of primary differentiation)&lt;br/&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体来说，就是产品做给谁、解决什么需求，产品是什么、核心价值是啥，跟竞争对手相比区别主要在哪。简单的 30 秒，但是这个定义过程需要做很多功课，非常耗时，但是如果能一口气讲清楚并且还能让人两眼发光，那么你就有了一个简单清晰而且还令人振奋的产品远景，也就有了一个好的开始。现在坐下来，好好思考一下，针对你的产品去填上那些括号里的内容，就像我对风车的远景定义。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;b&gt;For&lt;/b&gt; 创业团队&lt;br/&gt;
  &lt;b&gt;Who&lt;/b&gt; 想更好的成长&lt;br/&gt;
  &lt;b&gt;The&lt;/b&gt; 风车 &lt;b&gt;是一个&lt;/b&gt; 团队协作工具&lt;br/&gt;
  &lt;b&gt;That&lt;/b&gt; 以任务管理为基础，提供简洁纯净的工作空间，帮助团队节省大量时间和资源去做真正要做的事&lt;br/&gt;
  &lt;b&gt;Unlike&lt;/b&gt; Jira、Basecamp、Trello、Email 和 Excel&lt;br/&gt;
  &lt;b&gt;Our product&lt;/b&gt; 专注于任务进度掌控和高效执行，信息合理有效地组织起来，真正做到简单、轻量和高效的平衡。&lt;br/&gt;
&lt;/blockquote&gt;

&lt;p&gt;转换成中文习惯就是『风车是一款以任务管理为基础，提供简洁纯净的工作空间的团队协作工具，让创业团队能节省大量的时间和资源做真正重要的事情，更好的成长。不像 Jira、Basecamp、Trello、Email 或 Excel，风车更专注于任务的进度掌控和高效执行，信息合理有效地组织起来，真正做到简单、轻量和高效的平衡』。&lt;/p&gt;

&lt;p&gt;当有了这个远景以后，我们就应该围绕着这个远景去定义产品形态，设计出一个用户想用、用户知道怎么用并且可以做出来的产品&lt;sup id="fnref3"&gt;&lt;a href="#fn3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;。这个过程，需要整个团队一起参与和努力，更需要相互信任和尊重，以远景为中心，多从用户的角度出发和思考，慢慢聚拢收窄需求，最终的的目标是寻找到最小可行产品，用最小成本去做测试，看看我们是不是在正确的方向上。&lt;/p&gt;

&lt;p&gt;&lt;aside class=“aside”&gt;
  &lt;img alt="Discovery" src="http://yedingding.com/images/deliver-better-product-i/discovery.jpg?1404985052" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;MVP 的确定，绝对不是一蹴而就，而是需要一段时间的反复探索、试错和纠正，有时甚至需要原型的辅助测试。所以，在探索的过程中，我们需要不同时刻带不同的帽子。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;学会带最终用户的帽子。忘掉自己的知识背景，去思考如果自己是最终用户，会希望是什么样子的，尽可能的抛掉主观因素。&lt;/li&gt;
&lt;li&gt;学会带用户支持的帽子。去聆听用户的声音，他遇到了什么问题，他希望看到什么结果。即使是他要一辆更快的马车&lt;sup id="fnref4"&gt;&lt;a href="#fn4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;，对于我们来说也是知道他的问题是嫌弃速度不够快。&lt;/li&gt;
&lt;li&gt;学会带产品经理的帽子。用户声音中存在着很多噪音，用户也并不清楚自己需要什么，所以我们需要从众多的用户反馈和建议中挖掘出用户的真正潜在需求并给出可行解决方案。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;产品形态的定义过程，是一个从具象到抽象，再从抽象回到具象的过程。这个过程，依赖于对问题的深刻理解，也借助于用户行为测试和数据分析。 实际项目中，总会有很纠结的时候，听上去这个客户提的意见有道理，那个客户提的也有道理，我们该如何做决策？这个时候就需要依托于我们在前面定义的远景，通过做用户角色分析，来让我们专注在最重要的事情上。这将在本系列文章（II）里面介绍。&lt;/p&gt;

&lt;p&gt;在（I）的最后，再次重申，无论你的设计团队技术团队运营团队销售团队有多强，如果一开始设计出的不是一个用户想用且会用的产品，那么最终产品还是会走向失败。所以，从现在开始透明化你的产品远景，让团队每个成员都清楚知道前进的目标，这，真的很重要。&lt;/p&gt;

&lt;p&gt;读到这里，你有什么想法吗，欢迎大家留言讨论，谢谢!&lt;/p&gt;

&lt;p&gt;PS：上个月在杭州参加了吕毅的 CSPO 课程，有理论有实践，很有收获，推荐给产品经理或者创业者。如果你想了解吕毅和他的课程介绍，可以看&lt;a href="http://www.scrumalliance.org/community/profile/ylv"&gt;这里&lt;/a&gt;，9 月份在杭州也有一次课。&lt;/p&gt;

&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id="fn1"&gt;
&lt;p&gt;个人认为天时地理人和对应站在风口上，进入合适的市场和有一只强大的团队，当然这得有一些运气。&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn2"&gt;
&lt;p&gt;我得承认系列文章是很容易太监了，希望能多得到大家反馈和鼓励，希望不至于被批的太惨而心灰意冷，:)&amp;nbsp;&lt;a href="#fnref2" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn3"&gt;
&lt;p&gt;Valuable - 对用户有价值，用户想用；Usable - 用户知道怎么用；Feasible - 技术可以实现的。&amp;nbsp;&lt;a href="#fnref3" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn4"&gt;
&lt;p&gt;“If I had asked people what they wanted, they would have said faster horses.” - Henry Ford, the founder of the Ford Motor Company&amp;nbsp;&lt;a href="#fnref4" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>微信企业号，Do or Die?</title>
    <link rel="alternate" href="/2014/07/03/wechat-enterprise-challenge.html"/>
    <id>/2014/07/03/wechat-enterprise-challenge.html</id>
    <published>2014-07-03T20:00:00+08:00</published>
    <updated>2014-07-03T20:00:00+08:00</updated>
    <author>
      <name>叶玎玎</name>
    </author>
    <summary type="html">&lt;p&gt;&lt;img alt="WeChat" src="/images/wechat-enterprise-challenge/wechat-645x250.jpg?1404392414"&gt;&lt;/p&gt;

&lt;p&gt;对于一位在企业服务领域的创业者来说，最近几天最具有爆炸力的新闻莫如是微信继公众号、服务号之后，即将推出企业号，旨在连接人和企业。一时间，媒体、企业、创业者，都开始思考着各种可能性，到底企业号最后会是以什么形态出现，对于目前不瘟不火的企业服务市场又将会带来什么样的变化。&lt;/p&gt;

&lt;p&gt;“如果腾讯抄你们或者进入这个市场，你们会怎么办？”这是国内的 VC 们很喜欢问的一个问题。而现在腾讯是实实在在的进入了。腾讯的进入，对于像风车这样的标准化 SaaS 团队协作服务来说，到底会是一个机遇，还是一个灭顶之灾呢？&lt;/p&gt;

&lt;p&gt;从第一天知道这个消息的时候，我一直都是觉得这是一个好事，我非常乐于看到微信进入这个市场。不管我们承不承认，微信现在已经实实在在的渗透到了企业内部，使用粘度之高简直令人发指。如果在任何一个企业做一个调查，统计使用时间最多的办公软件，我相信微信都跳不出前三，即使它定位不在办公。公司员工之间在用微信沟通，公司客户之间在用微信沟通，创业者投资人之间也用微信在沟通，所以微信进入这个市场的时机已然成熟。&lt;/p&gt;

&lt;p&gt;那么，这个特殊的企业号能做什么？让我们先来看看服务号和公众号能做什么。风车目前在开发服务号，基本功能也是一些简单的快速任务入口和项目动态通知提醒，再在其上是提供服务信息和客服。这里最大的问题是服务号连接的是企业里的个人和企业服务提供商，而不是企业本身和企业服务提供商。如果我们要做到后者，就需要企业自身去申请服务号，然后跟风车做对接。而这，从传言来看，就是微信即将推出的企业号，唯一不同的是由微信来制定标准接口，类似苹果的应用商店，规定应用接入的规范，而风车只需要作为其中一个应用接入即可，事情将简单很多。而因此，企业也从多应用入口变成一个企业号入口。&lt;/p&gt;

&lt;p&gt;微信企业号的最大价值，在我看来主要是两点，不是它能做微信打卡，也不是它能做微信报销等，而是它解决了企业的两大基本问题，一是统一了账号系统，...&lt;/p&gt;</summary>
    <content type="html">&lt;p&gt;&lt;img alt="WeChat" src="http://yedingding.com/images/wechat-enterprise-challenge/wechat-645x250.jpg?1404392414" /&gt;&lt;/p&gt;

&lt;p&gt;对于一位在企业服务领域的创业者来说，最近几天最具有爆炸力的新闻莫如是微信继公众号、服务号之后，即将推出企业号，旨在连接人和企业。一时间，媒体、企业、创业者，都开始思考着各种可能性，到底企业号最后会是以什么形态出现，对于目前不瘟不火的企业服务市场又将会带来什么样的变化。&lt;/p&gt;

&lt;p&gt;“如果腾讯抄你们或者进入这个市场，你们会怎么办？”这是国内的 VC 们很喜欢问的一个问题。而现在腾讯是实实在在的进入了。腾讯的进入，对于像风车这样的标准化 SaaS 团队协作服务来说，到底会是一个机遇，还是一个灭顶之灾呢？&lt;/p&gt;

&lt;p&gt;从第一天知道这个消息的时候，我一直都是觉得这是一个好事，我非常乐于看到微信进入这个市场。不管我们承不承认，微信现在已经实实在在的渗透到了企业内部，使用粘度之高简直令人发指。如果在任何一个企业做一个调查，统计使用时间最多的办公软件，我相信微信都跳不出前三，即使它定位不在办公。公司员工之间在用微信沟通，公司客户之间在用微信沟通，创业者投资人之间也用微信在沟通，所以微信进入这个市场的时机已然成熟。&lt;/p&gt;

&lt;p&gt;那么，这个特殊的企业号能做什么？让我们先来看看服务号和公众号能做什么。风车目前在开发服务号，基本功能也是一些简单的快速任务入口和项目动态通知提醒，再在其上是提供服务信息和客服。这里最大的问题是服务号连接的是企业里的个人和企业服务提供商，而不是企业本身和企业服务提供商。如果我们要做到后者，就需要企业自身去申请服务号，然后跟风车做对接。而这，从传言来看，就是微信即将推出的企业号，唯一不同的是由微信来制定标准接口，类似苹果的应用商店，规定应用接入的规范，而风车只需要作为其中一个应用接入即可，事情将简单很多。而因此，企业也从多应用入口变成一个企业号入口。&lt;/p&gt;

&lt;p&gt;微信企业号的最大价值，在我看来主要是两点，不是它能做微信打卡，也不是它能做微信报销等，而是它解决了企业的两大基本问题，一是统一了账号系统，二是提供了工作入口。&lt;/p&gt;

&lt;p&gt;一个企业内部会使用到很多不同的应用，每个应用都可能会有其自成一套的用户系统，对于企业来说很是头疼。标准化登陆系统是有，比如 &lt;a href="http://en.wikipedia.org/wiki/LDAP"&gt;LDAP&lt;/a&gt;、&lt;a href="http://en.wikipedia.org/wiki/Saml"&gt;SAML&lt;/a&gt;、&lt;a href="http://www.ca.com/us/securecenter/ca-siteminder.aspx"&gt;Siteminder&lt;/a&gt; 或者 &lt;a href="http://en.wikipedia.org/wiki/Kerberos_(protocol)"&gt;Kerberos&lt;/a&gt; 这些 SSO 方案，但是一来小企业很少有能力上这种方案，二来非标准化下不同的企业有他自己的方案，每个都支持对于创业团队来说非常麻烦，所以员工账号系统的统一管理对于企业来说一直是个很大的痛点。但是，如果创业团队专门去做一个员工账号系统的服务，又会面临无人对接的困境，阻力大大。所以，微信的推出，一定程度上可以解决这个问题，企业服务只需要对接微信账号系统就可，而微信也控制了这个最最基本却又非常重要的员工入口。&lt;/p&gt;

&lt;p&gt;如果说企业里面有什么服务是必需品，那非沟通服务莫属。也许你们不用风车这样的任务管理工具，但是你一定会用微信这样的沟通工具；也许你们不用印象笔记这样的知识管理工具，但是你一定会用微信这样的沟通工具；也许你们不用 Yammer 这样的社交管理工具，但是你一定会用微信这样的沟通工具。沟通工具非常适合做企业工作的入口，来汇总分散在各地的信息。比如对于风车团队来说，我们使用 HipChat，里面会汇总我们在 BitBucket 上的代码提交信息，在 UserVoice 上的用户支持信息，我们的部署通知系统消息，我们的集成测试信息，我们的任务管理消息等等。沟通工具仍是沟通工具，我们不用它来当任务管理用，也不当知识管理用，但是它天然就是很好的信息聚合系统，也是员工可能一天在线最长的应用。这段时间国外大红大紫的 Slack&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; 便是同样的概念。&lt;/p&gt;

&lt;p&gt;所以，从上面两点来说，我觉得微信企业号的出现，对于企业服务创业者来说，更多的是机遇，而不是挑战。它非但不会压缩企业服务提供商的生存空间，而且会开放平台给企业开发者，可以让大家共同来构建生态圈，对于微信、企业和创业者来说都是三方共赢的好事。同时，因为腾讯的介入，对于创业者来说，企业服务市场的教育成本会大大降低。这一次，&lt;strong&gt;微信连接人和企业&lt;/strong&gt;&lt;sup id="fnref2"&gt;&lt;a href="#fn2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;

&lt;p&gt;目前对微信企业号的最大质疑在于员工不想与企业分享微信号，员工抗拒在微信中使用企业应用，因为&lt;del&gt;还让不让人下班了&lt;/del&gt;。姑且不说现在有些人就是用两个微信号的，一个加公司同事，另一个用来加个人朋友，单单说质疑微信企业号会造成员工压榨的情况，我只能说：“妈蛋，搞得好像没有微信企业号就不加班似的。看我口型，跟我喊，『用风车，不加班』，『用风车，不加班』。” 唉，讨厌，搞得我跟传销似的，敏捷宣言&lt;sup id="fnref3"&gt;&lt;a href="#fn3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;第一句，『个体和互动高于流程和工具』，尊重个体，加强团队之间的互动，然后才是合适的流程和相配合的工具。真用好了，我相信微信企业号及其生态圈可以成为中小企业成长的一把屠龙刀。&lt;/p&gt;

&lt;p&gt;你怎么看微信企业号，你们现在有用微信在做一些工作上的事情吗？欢迎留言交流。&lt;/p&gt;

&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id="fn1"&gt;
&lt;p&gt;&lt;a href="https://slack.com"&gt;Slack&lt;/a&gt; 是国外新进最火的企业沟通工具，没有之一，我不是他的用户，但是他的很多东西让我眼前一亮，后面专门写篇文章分析。&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn2"&gt;
&lt;p&gt;封闭不开放是腾讯一直的尿性，希望这次微信不要让我们失望&amp;hellip;.&amp;nbsp;&lt;a href="#fnref2" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn3"&gt;
&lt;p&gt;敏捷宣言中文版见&lt;a href="http://agilemanifesto.org/iso/zhchs/"&gt;这里&lt;/a&gt;&amp;nbsp;&lt;a href="#fnref3" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>Write the code. Change the world.</title>
    <link rel="alternate" href="/2014/06/24/write-the-code-change-the-world.html"/>
    <id>/2014/06/24/write-the-code-change-the-world.html</id>
    <published>2014-06-24T08:00:00+08:00</published>
    <updated>2014-06-24T08:00:00+08:00</updated>
    <author>
      <name>叶玎玎</name>
    </author>
    <summary type="html">&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="WWDC 2014" src="/images/write-the-code-change-the-world/WWDC14.png?1403585998" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Write the Code. Change the world.&amp;rdquo;, 这个标题是苹果这个月的 WWDC 上的宣传语，看得我很激动，也相信每一个耐不住寂寞的开发者内心里都会有这样的冲动。最近一两年间，越来越多的人开始把编码能力跟读写能力做比较，呼吁每个人都应该去学习编程序，认为这是以后每个人都需要掌握的基础技能。与之同时，黑客马拉松在国内也迅速的火起来，参加过几次，跟一群年轻人聚集在一起，交流想法并付诸于实践，在一个周末完成一个作品，每每想起都是段非常愉悦的经历。&lt;/p&gt;

&lt;p&gt;六一应 &lt;a href="http://segmentfault.com"&gt;SegmentFault&lt;/a&gt;&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; 高阳邀请，在其两周年黑客马拉松活动中作为技术创业者的代表，做一个分享。虽然作为创业者分享经验很不够格，不过这两年做风车下来也的确有一些想法，另外通过 &lt;a href="http://teahour.fm"&gt;Teahour&lt;/a&gt; 从不少创业者那里学到了很多，所以也就在活动上做了一次简短的即兴演讲，刚好跟 『Write the Code. Change the world』有点关联。&lt;/p&gt;

&lt;p&gt;SegmentFault 的使命是『创造属于开发者的时代』，我个人很喜欢这个远景，很美好。作为开发者，处在现在这个时代，我们是幸运的。我记得我 06 年的时候第一次跟朋友创业做互联网产品，当时什么东西都要自己去管，比如要自己买服务器、找 IDC 托管、产品开发中如果需要用什么服务都要自己做，真的是很讨厌，既浪费时间又浪费精力。而现在呢，相比较起来我们去制作一个产品的成本已经很低了，一个团队，三个月时间迅速开发原型，云主机用 UCloud 之类，文件存储用又拍云，发邮件用 SendCloud，还有各种开放平台等等。所以，作为一个开发者，作为一个会写代码的人&lt;sup id="fnref2"&gt;&lt;a href="#fn2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;，我们有个很大的优势，就是就算整个世界抛弃了你，你还是能一个人去把产品开发出来，启动成本很低。&lt;/p&gt;

&lt;p&gt;</summary>
    <content type="html">&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="WWDC 2014" src="http://yedingding.com/images/write-the-code-change-the-world/WWDC14.png?1403585998" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Write the Code. Change the world.&amp;rdquo;, 这个标题是苹果这个月的 WWDC 上的宣传语，看得我很激动，也相信每一个耐不住寂寞的开发者内心里都会有这样的冲动。最近一两年间，越来越多的人开始把编码能力跟读写能力做比较，呼吁每个人都应该去学习编程序，认为这是以后每个人都需要掌握的基础技能。与之同时，黑客马拉松在国内也迅速的火起来，参加过几次，跟一群年轻人聚集在一起，交流想法并付诸于实践，在一个周末完成一个作品，每每想起都是段非常愉悦的经历。&lt;/p&gt;

&lt;p&gt;六一应 &lt;a href="http://segmentfault.com"&gt;SegmentFault&lt;/a&gt;&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; 高阳邀请，在其两周年黑客马拉松活动中作为技术创业者的代表，做一个分享。虽然作为创业者分享经验很不够格，不过这两年做风车下来也的确有一些想法，另外通过 &lt;a href="http://teahour.fm"&gt;Teahour&lt;/a&gt; 从不少创业者那里学到了很多，所以也就在活动上做了一次简短的即兴演讲，刚好跟 『Write the Code. Change the world』有点关联。&lt;/p&gt;

&lt;p&gt;SegmentFault 的使命是『创造属于开发者的时代』，我个人很喜欢这个远景，很美好。作为开发者，处在现在这个时代，我们是幸运的。我记得我 06 年的时候第一次跟朋友创业做互联网产品，当时什么东西都要自己去管，比如要自己买服务器、找 IDC 托管、产品开发中如果需要用什么服务都要自己做，真的是很讨厌，既浪费时间又浪费精力。而现在呢，相比较起来我们去制作一个产品的成本已经很低了，一个团队，三个月时间迅速开发原型，云主机用 UCloud 之类，文件存储用又拍云，发邮件用 SendCloud，还有各种开放平台等等。所以，作为一个开发者，作为一个会写代码的人&lt;sup id="fnref2"&gt;&lt;a href="#fn2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;，我们有个很大的优势，就是就算整个世界抛弃了你，你还是能一个人去把产品开发出来，启动成本很低。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;但是，是否启动成本低，我们就应该甩开膀子干呢？千万别，思考清楚了，宁愿在真正开始做事情前多花时间思考。我在去年曾经写过一篇文章&lt;a href="http://yedingding.com/2013/10/11/how-to-find-tech-cofounder.html"&gt;『如何吸引技术合伙人』&lt;/a&gt;，其中很关键的两点是看对方是否对 idea 有足够的认识以及是否有 traction 可以验证。换位思考，我们自己创业时，也应该如此，多思考，好的坏的都要思考。&lt;/p&gt;

&lt;p&gt;去年中国 Ruby 大会上，Kevin Dewalt 曾经做过一次非常精彩的演讲 &lt;a href="http://www.infoq.com/cn/presentations/combination-ror-and-better-startup-strategy-in-business"&gt;『How to Use Rails and Ruby to validate your Startup idea』&lt;/a&gt;，他认为，一个产品的成功，在一开始就得思考这三个问题。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Problem - 问题？&lt;/strong&gt;. 你是否在解决一个真正的问题？你需要从第一天开始就去验证你的假设，这需要你去做产品定位和采访潜在用户。在没有找到这个答案之前，做其它任何的都是徒劳的。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Revenue - 会买？&lt;/strong&gt;. 我们得考虑商业价值，也就是人们是否愿意花钱解决这个问题？不管你在解决多少实际的问题，但是如果多数用户还是宁愿忍受这个问题也不愿意花钱买解决方案，那还是不值得做，得慎重。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Channel - 怎么卖&lt;/strong&gt;. 你有没有一个有效的渠道去推到愿意花钱解决这个问题的人？问题就在那里，也有足够的人愿意付钱，最后就是问问你能否推广到这个群体去，同时还能有盈利。如果你的推广成本大于你的收入，那么还是没法做。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以，如果你要开始一个项目，问自己这三个问题。思考清楚后，追随你的内心，做出选择，奋力向前。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="Maslow 5 层需求模型" src="http://yedingding.com/images/write-the-code-change-the-world/maslow-hierarchy-of-needs.jpg?1403585998" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;我那篇文章有个读者曾问过我，如果是我自己，创业最想得到的是什么。我的回答是，能自己主导做一些真正有社会价值的事，并且经济回报有盼头。我不是圣人，所以不否认是有物质的追求在，但是理想层面上的精神追求会更多点。马斯洛在其基本五层需求模型理论中指出，人类最底层的是衣食住行等生理需求，上一层是安全上的需求，第三层是作为社会群体在社交上的需求，第四层是尊重的需求，最上层是自我实现的需求，一种创造的需要。如果我们是工作的话，生理需求和安全需求会很容易满足。然后是社交需求、尊重需求的满足，最后我们才会考虑自我实现。但是创业不同，我认为创业是直接跳过了下面的四层，更多的是在追求自我实现的需求，去实现自己的理想和目标，所以愿意在一定时间内放弃自己的一些下层需求。&lt;/p&gt;

&lt;p&gt;我们都知道，创业成功的几率是非常小的，100 个项目里面有 90 个是失败的，有 8 个不失败但是也就那样，只有 2 个是非常成功的。Scott Shane&lt;sup id="fnref3"&gt;&lt;a href="#fn3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt; 在其著名畅销书 &lt;a href="http://www.amazon.com/The-Illusions-Entrepreneurship-Entrepreneurs-Investors/dp/0300158564"&gt;『The Illusions of Entrepreneurship』&lt;/a&gt; 就披露了很多关于创业者的黑暗事实，比如大多数创业公司会在 5 年内倒闭，大多数创业者比他工作的时候收入要低，大多数创业者收入不稳定，大多数创业者工作时间比工作要更长。理想很美好，现实很残酷，但是即使知道这些，我们是否就会失去创业的勇气？不！因为我们都有那么一点小理想，我们在追求自我实现，所以不如让我们姑且忘掉未来的成败，而是专注在成长和价值的提高上。而且，即使失败率如此之高，我们还是有办法来降低，比如更好的利用时间，更高效的做事，这也是我为啥坚持做风车的原因，风车本质上就是高效任务管理工具，让你们更好的成长，因为时间管理说白了到最后就是任务管理。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="Feature Usage" src="http://yedingding.com/images/write-the-code-change-the-world/feature-usage.jpg?1403585998" /&gt;&lt;br/&gt;
  &lt;small&gt;Source: Standish Group, Feature usage of software projects&lt;/small&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;对于大多数技术创业者来说，也许基本只有时间成本，可是时间对我们来说是一个非常昂贵的东西，所以要学会合理地利用时间和资源。钱是赚不完的，但是时间过了就找不回来了，这也是为什么我们要时刻提醒自己要专注在核心价值的地方。这并不是说我们应该工作的更努力，工作的时间应该要更长，而是努力用最小的时间去换取最大的价值。如果不是自己的产品价值，那么就不需要做，勇于对用户说不，价值少的可以考虑让第三方来做等等。Standish Group&lt;sup id="fnref4"&gt;&lt;a href="#fn4" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt; 曾在 2002 年做过一次市场调查，对于一个软件产品，平均竟然有 64% 的功能是用户从来不用或者很少用到的，而经常用到的只有 20%。所以，不要浪费时间精力去做那 64%！同时，比起不停的埋头开发，要不停的去了解用户，与用户沟通，保持改进，提高客户满意度，这些都是可以让我们降低失败率的手段。&lt;/p&gt;

&lt;p&gt;最后，以我自勉的几个字做结尾：『勿忘初心，但行好事，莫问前程』。&lt;/p&gt;

&lt;p&gt;PS：SegmentFault 2014 黑客马拉松北京站凑巧也在这个周末举行，有兴趣可以去参加一下，地址戳&lt;a href="http://segmentfault.com/e/hackathon-beijing-2014"&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id="fn1"&gt;
&lt;p&gt;SegementFault 是国内最活跃的黑客马拉松组织者。&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn2"&gt;
&lt;p&gt;现在学编程的门槛已经很低了，只要愿意并且投入时间就可。Udemy 上有不少非常好的课程，一般人我不告诉他。&amp;nbsp;&lt;a href="#fnref2" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn3"&gt;
&lt;p&gt;Scott Shane 是创业研究领域的著名学者、凯思西储大学管理学院教授，也是一位投资人。&amp;nbsp;&lt;a href="#fnref3" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn4"&gt;
&lt;p&gt;Standish Group 是美国专门从事跟踪 IT 项目成功或失败的权威机构，它每年会发布一个 CHAOS Report，给出IT项目相关调查数据结果。&amp;nbsp;&lt;a href="#fnref4" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>你是否关注过消费者心理？</title>
    <link rel="alternate" href="/2014/06/17/know-your-consumers.html"/>
    <id>/2014/06/17/know-your-consumers.html</id>
    <published>2014-06-17T12:00:00+08:00</published>
    <updated>2014-06-17T12:00:00+08:00</updated>
    <author>
      <name>叶玎玎</name>
    </author>
    <summary type="html">&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="消费者心理学" src="/images/know-your-consumers/consumer-psychology.jpg?1403056180"&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;上一期 &lt;a href="http://teahour.fm/2014/05/22/talk-about-finddog.html"&gt;Teahour&lt;/a&gt; 跟找狗网的创始人 James 聊起 2C 产品和 2B 产品的区别时，James 提到 2C 更多的是心理学操控，在人性的操控上，给大家多一些很奇怪的好处，比如最近在台湾很火的麦当劳早安闹钟应用，在你设立闹钟起床的同时，可以去麦当劳领取一份早餐优惠劵，又有一些好玩的点让你去分享这份经历，成为热门新闻话题，瞬间引爆，上架首日就冲上 App Store 免费排行榜第一名，没有刷榜喔。仔细思考这个案例其实很有意思，里面有很多的心理学及设计考量，而这些往往是我很少关注的。&lt;/p&gt;

&lt;p&gt;移动平台、实时技术、社会化媒体、微信营销、O2O，过去几年这些名词发展的很快，给创业者带来了很大的机会，也带来了新的挑战。那是否堆砌几个关键词就能成功了？显然不是，用户根本不关心这些新概念抑或新特性，如何能用这些来提高用户满意度，影响用户的购买决策过程，才是真正的关键，而这，既是消费者心理学，又是社会心理学。&lt;/p&gt;

&lt;p&gt;百度百科对&lt;a href="http://baike.baidu.com/subview/165512/5093838.htm"&gt;消费者心理学&lt;/a&gt;的介绍中，提到了人类行为的几种常见心理以及不同类型的人群的消费心理差别。让我们回想一下自己最近几次的消费经历，不管是软件、服务还是实物，你为什么要买这个东西？你是非常理智的进行消费行为的吗？你是否受到外界因素的影响？你是否属于冲动型消费？大多数情况下，我们并不如我们自己想的那么理智，我们的消费行为是受到情感因素的影响，在一定的推波助澜下做出的决定，当然，我们后面会给自己找出千百种理由，来说服自己为何这次消费是合理的。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="农夫山泉" src="/images/know-your-consumers/nongfusanquan.jpg?1403056180"&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;这是农夫山泉的一则广告，营销其实也是如此。我们并不是要去也不能创造出消费者的一个需求，而是去激发他未开发或者未意识到的需求，驱使他做出消费决策。如何去寻找到这个需求并且刺激用户，正是分辨好的营销人员和差的营销人员的差别。Simon Sinek 曾在 TEDx 上做过一次非常棒的分享&lt;a href="http://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action"&gt;『How great leaders...&lt;/a&gt;&lt;/p&gt;</summary>
    <content type="html">&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="消费者心理学" src="http://yedingding.com/images/know-your-consumers/consumer-psychology.jpg?1403056180" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;上一期 &lt;a href="http://teahour.fm/2014/05/22/talk-about-finddog.html"&gt;Teahour&lt;/a&gt; 跟找狗网的创始人 James 聊起 2C 产品和 2B 产品的区别时，James 提到 2C 更多的是心理学操控，在人性的操控上，给大家多一些很奇怪的好处，比如最近在台湾很火的麦当劳早安闹钟应用，在你设立闹钟起床的同时，可以去麦当劳领取一份早餐优惠劵，又有一些好玩的点让你去分享这份经历，成为热门新闻话题，瞬间引爆，上架首日就冲上 App Store 免费排行榜第一名，没有刷榜喔。仔细思考这个案例其实很有意思，里面有很多的心理学及设计考量，而这些往往是我很少关注的。&lt;/p&gt;

&lt;p&gt;移动平台、实时技术、社会化媒体、微信营销、O2O，过去几年这些名词发展的很快，给创业者带来了很大的机会，也带来了新的挑战。那是否堆砌几个关键词就能成功了？显然不是，用户根本不关心这些新概念抑或新特性，如何能用这些来提高用户满意度，影响用户的购买决策过程，才是真正的关键，而这，既是消费者心理学，又是社会心理学。&lt;/p&gt;

&lt;p&gt;百度百科对&lt;a href="http://baike.baidu.com/subview/165512/5093838.htm"&gt;消费者心理学&lt;/a&gt;的介绍中，提到了人类行为的几种常见心理以及不同类型的人群的消费心理差别。让我们回想一下自己最近几次的消费经历，不管是软件、服务还是实物，你为什么要买这个东西？你是非常理智的进行消费行为的吗？你是否受到外界因素的影响？你是否属于冲动型消费？大多数情况下，我们并不如我们自己想的那么理智，我们的消费行为是受到情感因素的影响，在一定的推波助澜下做出的决定，当然，我们后面会给自己找出千百种理由，来说服自己为何这次消费是合理的。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="农夫山泉" src="http://yedingding.com/images/know-your-consumers/nongfusanquan.jpg?1403056180" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;这是农夫山泉的一则广告，营销其实也是如此。我们并不是要去也不能创造出消费者的一个需求，而是去激发他未开发或者未意识到的需求，驱使他做出消费决策。如何去寻找到这个需求并且刺激用户，正是分辨好的营销人员和差的营销人员的差别。Simon Sinek 曾在 TEDx 上做过一次非常棒的分享&lt;a href="http://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action"&gt;『How great leaders inspire action』&lt;/a&gt;，介绍了他提出的黄金圈法则。其中很简单的一点是，用户并不为你做的东西买单，而是用户在为你&lt;strong&gt;为什么(Why)你要做这个东西&lt;/strong&gt;买单，即产品创造的使命。所以优秀的市场营销者，会先去传递产品的使命，然后才是与使命匹配的具体需求实现，由内而外地与消费者建立建立信任关系&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="Golden Circle" src="http://yedingding.com/images/know-your-consumers/golden-circle.png?1403056180" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;上个月，老罗的锤子手机发布会便是一次非常成功的消费者心理操控案例，其中有两个非常具有传播性的点。从理想主义者到『我不是为了输赢，我就是认真！！』的工匠精神，和『令科技巨头蒙羞的 100 万元的 OpenSSL 基金会捐款』 ，都瞬间击中感动党泪点。后面无论是微博上还是微信上的晒单，都能感受到这些感动党那股跟老罗一样的”认真劲”。而这些人，如果我们仔细分析锤子手机的定价策略和营销手段，不难看出正是老罗要的第一批客户。老罗的第一步，不需要颠覆，不需要证明自己是乔帮主第二，他需要的是足够的订单，给自己给团队争取时间，让他能在未来进入大众市场，做出真正心目中的手机。所以，他去做用户分类，去做产品定位，然后把产品展示到这群用户面前。第一步，很成功！&lt;/p&gt;

&lt;p&gt;了解你的用户群体并做分类，这正是所有的 Customer Development 的书一再在强调的，因为所有的心理分析和用户行为分析都必须基于典型群体，从共性中找差异性，从差异性中找共性。生活中我们不可能做到取悦所有人，在销售上也一样。如果我们尝试让产品去适合所有人，结果必然是没有人会需要这样的产品，但是一旦当你清晰的面向某个细分人群，事情就会变简单了，有的放矢，对群体挖得越深，就越能针对性的采用一些策略来介入用户的决策过程。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you want to convert more people into customers, try to convert a smaller number of better-suited people.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;再说产品定位和定向营销，如果说营销上的扩散传播是用户获取的手段的话，那么当用户打开你的应用或者来到你的站点后那几十秒钟，则是另外一个战场，你需要在异次元空间清晰的跟访客进行沟通，来达到注册转化和用户留存&lt;sup id="fnref2"&gt;&lt;a href="#fn2" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;的目的，所以你需要设身处地的从用户的需求出发考虑。然后，争取用文字图像甚至视频去刺激用户，诱发共鸣，让用户产生消费冲动。而这个消费动机的引发不在于你做了什么，没有人关心你做了什么&lt;sup id="fnref3"&gt;&lt;a href="#fn3" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;，无论你是多么认真多么专业，用户关心的永远是他的需求是否得到了满足，也就是你做的东西是否解决了他的问题，所以，你要向用户传达的是产品所能给他提供的利益和价值，而不是产品的功能。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;People don&amp;rsquo;t want to buy a quarter-inch drill. They want a quarter-inch hole!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里插播一个广告，因为我自己本身在企业服务这个领域创业，而企业消费市场跟个人消费市场又有很大的不同。大部分时候企业服务产品，使用产品的用户和做出决策购买产品的客户不是同一个人，甚至使用者都不参与到购买过程。所以为了优先满足付钱的人的需求，很多企业服务公司根本不关心终端用户用的爽不爽，这也是目前绝大多数企业软件完全没有用户体验可言的原因。然而，现在游戏规则正在慢慢改变，&lt;a href="http://en.wikipedia.org/wiki/Bring_your_own_device"&gt;BYOD&lt;/a&gt; 已经不再只是未来的趋势，而是已经实实在在发生在我们身边企业里的事实。现在很难想象一个公司能强制要求员工必须用什么，不能用什么，如果这样最后的结果也只能是上下敷衍。所以我一开始做风车，我必须保证的是首先这应该是一款让每天真正在用的人感觉特别爽的工具，自下而上去推行，虽说不易，但是却能创造更忠诚的用户，更有效。就如 Evernote 的 CEO Phil Libin 所言，&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;即便是企业产品也应以取悦个人用户为目标。UI 和 UX 设计在未来企业产品设计中将越来越重要，只有这样，员工才会违反公司规定来使用你们的产品。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="推荐书籍" src="http://yedingding.com/images/know-your-consumers/books.png?1403056180" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;技术在不停进步的同时，人也在进步，现在的消费者已经越来越少被电视报纸等传统轰炸式营销方式所影响，我们需要从更深层次的用户需求出发来参与到消费决策过程。所以，当我们在花时间创造一个卓越产品的同时，也请不要忘了去打造卓越的用户体验，不仅是产品使用上的，也需要是消费过程中的。学会如何打造和培养卓越的用户体验对你产品的成功是至关重要的。一篇学习过程中简单的随想，希望能给你带来一些不同的想法，后面再跟大家分享更多一些具体的实战经验。如果你有兴趣了解更多，推荐一本书，来自 Robert Cialdini 的 &lt;a href="http://www.amazon.com/Influence-Practice-Robert-B-Cialdini/dp/0205609996"&gt;Influence: Science and Practice&lt;/a&gt;。&lt;/p&gt;

&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;

&lt;li id="fn1"&gt;
&lt;p&gt;俗称&amp;ldquo;宗教粉&amp;quot;、&amp;quot;脑残粉&amp;quot;。&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn2"&gt;
&lt;p&gt;参考 David McClure 的 &lt;a href="http://baike.baidu.com/view/10197444.htm"&gt;AARRR&lt;/a&gt; 模型，在『创业成长，从分析开始』(&lt;a href="http://yedingding.com/2014/03/27/growth-from-analytics.html"&gt;http://yedingding.com/2014/03/27/growth-from-analytics.html&lt;/a&gt;) 中也有介绍。&amp;nbsp;&lt;a href="#fnref2" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;li id="fn3"&gt;
&lt;p&gt;也许是错误的，你的父母朋友关心，:)&amp;nbsp;&lt;a href="#fnref3" rev="footnote"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;

&lt;/ol&gt;
&lt;/div&gt;
</content>
  </entry>
  <entry>
    <title>我们是如何使用风车的</title>
    <link rel="alternate" href="/2014/05/20/how-fengche-use-fengche-to-build-fengche.html"/>
    <id>/2014/05/20/how-fengche-use-fengche-to-build-fengche.html</id>
    <published>2014-05-20T12:00:00+08:00</published>
    <updated>2014-05-20T12:00:00+08:00</updated>
    <author>
      <name>叶玎玎</name>
    </author>
    <summary type="html">&lt;h3&gt;前言&lt;/h3&gt;

&lt;p&gt;随着使用风车的团队越来越多，不少人都问我有没有一些风车使用最佳实践，很想知道我们风车团队自己是怎么使用风车这个工具的。所以在这里我就介绍一下我们的使用方式，整体来说，因为团队性质原因，会更加贴近产品开发团队。&lt;/p&gt;

&lt;h3&gt;背景&lt;/h3&gt;

&lt;p&gt;任何离开背景的使用方法都是没有意义的，所以在介绍我们如何使用风车前，先介绍一下我们的背景，如果你读过我之前写的一些文章的话，你可能会有所了解，我们是一个远程开发团队，每个成员都在不同的地方，主要通过在线沟通，所以在进度管理和沟通交流上很需要一个管理工具，对远程有兴趣的可以参看我的这篇文章 &lt;a href="http://yedingding.com/2013/07/24/remote-team-the-things-you-should-know.html"&gt;『远程工作经验谈 - 如何适应及如何管理』&lt;/a&gt;。二是风车的诞生初衷是为了解决我自己在项目管理上的问题的。我的上一份工作是一个企业社交工具的技术负责人，需要做项目管理的工作，我们前前后后使用过不少的工具，非常不尽如人意，我每天需要在工具上浪费一两个小时，非常痛苦，所以就萌生了自己做一个刚刚好的工具，风车就是这么来的。可以说，风车一开始是专门为开发人员打造的项目管理工具。尽管现在更加的普适，适用于任何流程化任务管理的场景，风车还是非常适合开发团队做项目管理用。&lt;/p&gt;

&lt;p&gt;</summary>
    <content type="html">&lt;h3&gt;前言&lt;/h3&gt;

&lt;p&gt;随着使用风车的团队越来越多，不少人都问我有没有一些风车使用最佳实践，很想知道我们风车团队自己是怎么使用风车这个工具的。所以在这里我就介绍一下我们的使用方式，整体来说，因为团队性质原因，会更加贴近产品开发团队。&lt;/p&gt;

&lt;h3&gt;背景&lt;/h3&gt;

&lt;p&gt;任何离开背景的使用方法都是没有意义的，所以在介绍我们如何使用风车前，先介绍一下我们的背景，如果你读过我之前写的一些文章的话，你可能会有所了解，我们是一个远程开发团队，每个成员都在不同的地方，主要通过在线沟通，所以在进度管理和沟通交流上很需要一个管理工具，对远程有兴趣的可以参看我的这篇文章 &lt;a href="http://yedingding.com/2013/07/24/remote-team-the-things-you-should-know.html"&gt;『远程工作经验谈 - 如何适应及如何管理』&lt;/a&gt;。二是风车的诞生初衷是为了解决我自己在项目管理上的问题的。我的上一份工作是一个企业社交工具的技术负责人，需要做项目管理的工作，我们前前后后使用过不少的工具，非常不尽如人意，我每天需要在工具上浪费一两个小时，非常痛苦，所以就萌生了自己做一个刚刚好的工具，风车就是这么来的。可以说，风车一开始是专门为开发人员打造的项目管理工具。尽管现在更加的普适，适用于任何流程化任务管理的场景，风车还是非常适合开发团队做项目管理用。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h3&gt;计划&lt;/h3&gt;

&lt;p&gt;对于风车，我们有很多计划，比如日历、自定义状态等。但是毕竟资源所限，事情得一个一个来做，所以我们得区分长期计划和短期计划。对应到风车里，我们大量的使用收集箱和任务列表，如下图。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="Planning" src="http://yedingding.com/images/how-fengche-use-fengche-to-build-fengche/planning.png?1400582492" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;风车是项目、任务列表和任务的三层结构。任务列表作为中间很重要的一环，是任务的集合，用来组织和管理任务。对开发来说，任务列表主要就是迭代周期。所以，我们会有两类非常重要的列表，一是当前迭代列表，二是短期计划列表。当前迭代列表如上图的&lt;strong&gt;『Dashboard Stats』&lt;/strong&gt;和&lt;strong&gt;『May Day』&lt;/strong&gt;，短期计划列表如上图的&lt;strong&gt;『六月工作计划』&lt;/strong&gt;。这里之所以有两个当前迭代列表是因为『Dashboard Stats』是一个大且独立的功能，会有很多小任务，需要多人协作，所以我们单独拿出来跟踪，而『May Day』是真正的当前迭代列表。至于『六月工作计划』，顾名思义，就是我们计划在下个月做的一些事情。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;『收集箱』&lt;/strong&gt;是另外一个很重要的列表，用来收集暂时还没有计划、零散的想法，比如一些未来有可能会加的功能。这里主要供我们做头脑风暴，当跟用户交流时有一些有意思的点后，就在收集箱记录下来，但是暂时不在当前周期做任何评估。等到了新的迭代开始前，我会过一遍收集箱，调整优先级，把一些能开始做的放入到短期计划列表中。&lt;/p&gt;

&lt;p&gt;也许这时你有个新的问题，『收集箱』和『六月工作计划』是否有冲突或者冗余？这两者还是很不一样的。收集箱里存放的更多是用户故事这种，从用户角度出发的需求定义，粒度大，把一件事情说清楚就可以了。而六月执行计划里任务则从用户故事细化，变成可执行的任务，同时，这个列表也用来记录一些不太紧急的 bug。&lt;/p&gt;

&lt;p&gt;所以当做计划时，事情就方便了，可以直接在『六月工作计划』里选择优先级高的任务，移动到新的当前迭代列表就可以了。与此同时，会对任务做一些简单的评估以及分配，确保工作量合理。而已经完成的任务列表，此时已经可以归档掉了。&lt;/p&gt;

&lt;h3&gt;执行&lt;/h3&gt;

&lt;p&gt;计划后就是执行期了，流程化的任务执行是风车的核心，让正确的人能在正确的时间去做正确的事情。要做到这点，首先必须确保大家的信息对称和一致，任何人都知道项目里发生了什么，事情做到了哪一步了。所以，我们要求任何人开始做某个任务的时候，都必须更新状态到『进行中』，同时，同时在进行中的任务最好不要超过一个。如果任务完成了，需要更新状态到『已完成』，可以让团队其他成员进行验收。所以，我随时都可以了解当前迭代进行到什么状态了，团队每个人正在做什么，哪些任务已经完成等待验收，哪些任务已经验收通过部署到线上，哪些任务还需要完成，如下图。同时，事情有轻重缓急，有重要不重要、紧急不紧急两个维度，所以我们约定待办列表里越上面的任务越紧急，同时在任务详情里设置重要或者不重要。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="任务清单" src="http://yedingding.com/images/how-fengche-use-fengche-to-build-fengche/center.png?1400582492" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;每个任务在待办的状态时并不一定会被分配给某位成员，我们更主张主动的去领任务，看看有什么自己可以做的，包括即使已经被分配给别人了。除了负责人以外，有些任务需要多人协作，比如上图 Roy 在实现『下载所有附件』的功能时，需要我这边提供一些前端实现上的帮忙，所以我会被 Roy 加入到协作者列表里面，这样这个任务发生的任何变化都会实时的通知到 Roy 和我，包括动态和讨论。关于协作者这个功能的用法，可以参考 Roy 写得这篇文章，&lt;a href="https://fengcheco.com/blog/new-feature-task-followers/"&gt;『新功能：协作者』&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="任务详情" src="http://yedingding.com/images/how-fengche-use-fengche-to-build-fengche/details.png?1400582492" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;讨论是另一个工作上非常重要的部分。我们是远程团队，大部分沟通都发生在线上，即使在线下我也不主张随意的去打断同事，见我之前写的文章 &lt;a href="http://yedingding.com/2013/12/03/improve-productivity-in-office.html"&gt;『不要让办公室成为你的效率杀手』&lt;/a&gt;。我们会尽量把跟任务执行相关的事情都放到风车里讨论，主要是出于两点考虑。首先是任何讨论都是按照主题划分的，对比用 QQ 这些聊天工具，实时的交流体验一样，但是主题明确，不会错乱，过程和结论也都有记录，随时可以回顾和查找。同时，对比交流直接靠吼，虽然讨论是同步推送到所有团队成员那里，但是可以异步处理，没有打断别人，可以等到完成手上工作后才去处理。有时如果需要把某个人拉进来讨论一些 feature，直接用 @ mention 就可以了，对方就会收到实时的通知。&lt;/p&gt;

&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="任务讨论" src="http://yedingding.com/images/how-fengche-use-fengche-to-build-fengche/activity.png?1400582492" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;p&gt;作为产品团队，时常会有一些新的需求进来，亦或是一些 bug。我们是允许随时有新的任务进来的，但是如果是哪种迟一天做早一天做关系不大的任务，就没必要添加进来打乱节奏了。&lt;/p&gt;

&lt;h3&gt;发布&lt;/h3&gt;

&lt;p&gt;作为产品团队，只有发布才是带来最终价值的，这也是我坚持做持续发布，而不是等整个迭代结束后统一发布的原因。另外，我们的代码质量更多的依赖代码审核和自动化测试，而不是放在 staging 环境做人肉测试，除非是一些比较大拿不准的变化。我在之前的文章&lt;a href="http://yedingding.com/2013/09/11/practical-git-flow-for-startups.html"&gt;『实用 GIT 工作流』&lt;/a&gt;介绍了我们团队使用的代码工作流，简单说，就是必须随时有一个可发布的分支，对应风车是 master 分支，同时，工作分支对应风车里的具体任务，跟 git commits 结合起来，同时能很简单很早的 merge 过来。我们的代码审核流程可以阅读这篇文章&lt;a href="http://yedingding.com/2013/08/08/dig-into-code-review-process.html"&gt;『让代码审查成为你的团队习惯』&lt;/a&gt;。&lt;/p&gt;

&lt;h3&gt;总结&lt;/h3&gt;

&lt;p&gt;回到风车的设计初衷，我们就是想设计一个刚刚好的工具，即使以后会有所扩展，但是一致的原则是用户使用仍然要简单高效，如果违背了这个原则，那么宁愿砍功能。我希望用户在使用时能专注做事，不要被冗余的东西所干扰，分散注意力或者带来额外的操作成本，这也是我们在可视范围只放必要东西的原因，让用户可以节省时间，更多的花在做事执行上。尽管用户会有各种各样的要求，希望能加这个功能那个功能，但是我会非常慎重的去筛选，我真的受够了过去在那些复杂的工具上浪费一两个小时！！！&lt;/p&gt;

&lt;p&gt;如果说风车只有一个价值，那就是能让你的团队真正的不断优化工作流程。偶尔能听到一些人说人是最重要的，流程和工具无所谓，我只能说声呵呵，要不你就是能力没到，要不你就是没用过好的工具。一个简单的工具加上一个优化的流程能让整个团队事半功倍，这也是为什么在敏捷项目管理过程中会单独有个非常重要的步骤 &lt;strong&gt;Retrospective&lt;/strong&gt;，正是为了迭代改进流程。当你有了合适的人和合适的流程，千万不要忽略合适的工具，它真的会带来很大的不同，无论是不是风车。&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>采访《七周七数据库》作者 Eric Redmond - 数据库的故事 </title>
    <link rel="alternate" href="/2014/05/15/database-with-eric-redmond.html"/>
    <id>/2014/05/15/database-with-eric-redmond.html</id>
    <published>2014-05-15T13:04:00+08:00</published>
    <updated>2014-05-15T13:04:00+08:00</updated>
    <author>
      <name>叶玎玎</name>
    </author>
    <summary type="html">&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="Sever Database in Seven Weeks" src="/images/database-with-eric-redmond/database.jpg?1400134984"&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;blockquote&gt;
  目前 Teahour 的网站不适合放文本，需要重新设计，暂时先放我自己博客上。
&lt;/blockquote&gt;

&lt;p&gt;本期音频文本非常感谢&lt;a href="http://weibo.com/wuyicun"&gt;@吴怡村&lt;/a&gt;的整理。本文是 Teahour 第 18 期 &lt;a href="http://teahour.fm/2013/06/03/databases-with-eric-redmond.html"&gt;『Interview with Eric Redmond about Database』&lt;/a&gt; 的录音文本，欢迎大家订阅 Teahour，iTunes URL 是 &lt;a href="http://itunes.apple.com/cn/podcast/teahour.fm/id608387170?l=en"&gt;http://itunes.apple.com/cn/podcast/teahour.fm/id608387170&lt;/a&gt;。Android 用户可以使用 &lt;a href="http://m.coolapk.com/apk/de.danoeh.antennapod"&gt;AntennaPod&lt;/a&gt; 来订阅。同时，欢迎加 Teahour 好友，&lt;a href="http://weibo.com/teahourfm"&gt;微博&lt;/a&gt;和 &lt;a href="https://twitter.com/teahourfm"&gt;Twitter&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;audio controls name="media"&gt;
  &lt;source src="http://screencasts.b0.upaiyun.com/podcasts/teahour_episode_18.m4a" type="audio/mpeg"&gt;
&lt;/source&gt;&lt;/audio&gt;&lt;/p&gt;

&lt;h4&gt;Part 1: Introduction and the CAP theorem&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Welcome everybody! In this episode, we have &lt;a href="http://coderoshi.com/"&gt;Eric Redmond&lt;/a&gt; with us. I’m your host &lt;a href="http://knwang.com/"&gt;Kevin&lt;/a&gt; and we also have &lt;a href="http://yedingding.com/"&gt;Dingding&lt;/a&gt;. Eric, why don’t you introduce yourself first?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Sure, as you’ve mentioned, I’m Eric Redmond. I’m relevant to this episode. I co-authored a book with &lt;a href="https://twitter.com/hexlib"&gt;Jim Wilson&lt;/a&gt; called &lt;a href="http://pragprog.com/book/rwdata/seven-databases-in-seven-weeks"&gt;Seven Databases in Seven Weeks&lt;/a&gt;. And the idea was that with all the changes that have been recurring in the database marketplace over the past few years, it was probably the...&lt;/p&gt;</summary>
    <content type="html">&lt;p&gt;&lt;aside class="aside"&gt;
  &lt;img alt="Sever Database in Seven Weeks" src="http://yedingding.com/images/database-with-eric-redmond/database.jpg?1400134984" /&gt;
&lt;/aside&gt;&lt;/p&gt;

&lt;blockquote&gt;
  目前 Teahour 的网站不适合放文本，需要重新设计，暂时先放我自己博客上。
&lt;/blockquote&gt;

&lt;p&gt;本期音频文本非常感谢&lt;a href="http://weibo.com/wuyicun"&gt;@吴怡村&lt;/a&gt;的整理。本文是 Teahour 第 18 期 &lt;a href="http://teahour.fm/2013/06/03/databases-with-eric-redmond.html"&gt;『Interview with Eric Redmond about Database』&lt;/a&gt; 的录音文本，欢迎大家订阅 Teahour，iTunes URL 是 &lt;a href="http://itunes.apple.com/cn/podcast/teahour.fm/id608387170?l=en"&gt;http://itunes.apple.com/cn/podcast/teahour.fm/id608387170&lt;/a&gt;。Android 用户可以使用 &lt;a href="http://m.coolapk.com/apk/de.danoeh.antennapod"&gt;AntennaPod&lt;/a&gt; 来订阅。同时，欢迎加 Teahour 好友，&lt;a href="http://weibo.com/teahourfm"&gt;微博&lt;/a&gt;和 &lt;a href="https://twitter.com/teahourfm"&gt;Twitter&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;audio controls name="media"&gt;
  &lt;source src="http://screencasts.b0.upaiyun.com/podcasts/teahour_episode_18.m4a" type="audio/mpeg"&gt;
&lt;/audio&gt;&lt;/p&gt;

&lt;h4&gt;Part 1: Introduction and the CAP theorem&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Welcome everybody! In this episode, we have &lt;a href="http://coderoshi.com/"&gt;Eric Redmond&lt;/a&gt; with us. I&amp;rsquo;m your host &lt;a href="http://knwang.com/"&gt;Kevin&lt;/a&gt; and we also have &lt;a href="http://yedingding.com/"&gt;Dingding&lt;/a&gt;. Eric, why don&amp;rsquo;t you introduce yourself first?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Sure, as you&amp;rsquo;ve mentioned, I&amp;rsquo;m Eric Redmond. I&amp;rsquo;m relevant to this episode. I co-authored a book with &lt;a href="https://twitter.com/hexlib"&gt;Jim Wilson&lt;/a&gt; called &lt;a href="http://pragprog.com/book/rwdata/seven-databases-in-seven-weeks"&gt;Seven Databases in Seven Weeks&lt;/a&gt;. And the idea was that with all the changes that have been recurring in the database marketplace over the past few years, it was probably the single best way we knew to try and communicate all of these changes: by just teaching different databases that correspond to different styles. So in the book, as an example of relational databases, we had &lt;a href="http://www.postgresql.org/"&gt;Postgres&lt;/a&gt;, and as an example of key-value storage of two different kinds; we had &lt;a href="https://basho.com/riak"&gt;Riak&lt;/a&gt; and &lt;a href="http://redis.io/"&gt;Redis&lt;/a&gt;; for column-oriented data stores we had &lt;a href="http://hbase.apache.org/"&gt;HBase&lt;/a&gt; and a couple document data stores with &lt;a href="http://couchdb.apache.org/"&gt;Couch&lt;/a&gt; and &lt;a href="http://www.mongodb.org/"&gt;Mongo&lt;/a&gt;, and a graph data store with &lt;a href="http://www.neo4j.org/"&gt;Neo4j&lt;/a&gt;. Other than that, I actually work for a company &lt;a href="https://basho.com"&gt;Basho&lt;/a&gt; that makes one of the databases: Riak. Strangely enough, until I wrote this book, I had not even heard of it and I was so impressed with the design. I even mentioned in the book that out of all of these databases it was the one that struck me as one of the most architecturally elegant of the distributed styles that I just couldn&amp;rsquo;t say enough positive things. Finally I was just saying so many positive things I guess they decided to pay me for it. So now I work at Basho.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK, so maybe (let&amp;rsquo;s talk about) something about your background. Why are you all of a sudden interested in databases?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Well, like most things. It seems like all of a sudden but there&amp;rsquo;s a long and boring history behind it. No, I&amp;rsquo;ve actually been working in databases since college at &lt;a href="http://www.purdue.edu/"&gt;Purdue&lt;/a&gt;. I worked in (of course at that time, relational databases were the only real options anybody had) specifically Postgres; that&amp;rsquo;s where I did a lot of my work and even more specifically than that, high-dimensional indexing: the ability to index multimedia data by mapping it to a point in n-dimensional space was the focus of our research at the time. It&amp;rsquo;s interesting now in retrospect how almost silly that was. There are so many more efficient ways of mapping high-dimensional data than simply plotting a point in 5,000 dimensional features of vector space and doing a nearest neighbour search on it but it was still really interesting. It was sort of my first foray into this world as an undergrad.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK, so you actually did some research on database in university.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Whoa, whoa (let&amp;rsquo;s) back up. My name is not on any papers. I definitely wouldn&amp;rsquo;t go as far as saying I did research. I was an undergrad that helped with a lot of this research though.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK, just to be accurate on the record.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dingding&lt;/strong&gt;: This is Dingding. So (about) your book &lt;em&gt;Seven Database in Seven Weeks&lt;/em&gt;, I&amp;rsquo;m just curious that why it&amp;rsquo;s seven, not six and not eight?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: That&amp;rsquo;s a good question. Actually, there was a book preceeding this by an author Bruce Tate called &lt;a href="http://pragprog.com/book/btlang/seven-languages-in-seven-weeks"&gt;Seven Languages in Seven Weeks&lt;/a&gt; and the theory was very similar that if you learn all these different types of programming languages that will give you a very good overview of the programming language ecosystem. He&amp;rsquo;s the one that picked the number and we just stuck with it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: That actually is a very nice book and very mind-expanding. I think your book will be following a very similar pattern: just seven representative databases in different genres.&lt;/p&gt;

&lt;p&gt;So when you graduated, did you start as a database guy?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: No, I graduated right at the end of the dotcom boom: right when the dotcom era crashed (at) end of 2002, so I got whatever job I could. As much as I would have love to get into data management, I actually try to interview with Google. At that time, they had less than 100 employees but since I didn&amp;rsquo;t have a PhD or anything, they weren&amp;rsquo;t really interested. Although big data management, even at that time, was something I was very interested in. But I just took whatever job I could and I ended up being a software engineer for a few years and finally I was able to fall back into the database world: originally working for a company that does Mongo hosting called &lt;a href="https://www.mongohq.com/"&gt;MongoHQ&lt;/a&gt; then from there moved on over to Basho to work on Riak.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK. I want to start this discussion of databases with the &lt;a href="http://en.wikipedia.org/wiki/CAP_theorem"&gt;CAP theorem&lt;/a&gt;; I actually heard this from you. I think that would lay a pretty good foundation when we talk about each genre of databases. Why don&amp;rsquo;t you talk about this CAP theorem?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Sure. There&amp;rsquo;s a lot of theory to know about distributed systems in general and those that manage data in particular. A lack of knowledge of these details have been bringing people to make wild claims like 100% up-time guarantees or what I think is even crazier that is that CAP doesn&amp;rsquo;t matter. You&amp;rsquo;re starting to hear this trope quite often now. Really, the cornerstone of all distributed databases is the CAP theorem. The CAP theorem: C stands for consistency, A stands for availability and P is partition-tolerance. I&amp;rsquo;ll start with partition-tolerance first. This means a system that can tolerate a network partition, meaning lost network packets, which is always a possibility. Effectively, partition-tolerance means your system is distributed. If it&amp;rsquo;s possible to have two computers on a network where one tries to communicate to another one and that signal can be lost. If it can be lost, that&amp;rsquo;s a network partition. In the simplest case where you have two computers and one cable running between them, if you just take a pair of scissors and you cut that cable, you now have two computers that are effectively their own little sub-networks because they can&amp;rsquo;t talk to each other any more. You can heal that partition eventually (take a bunch of electrical tape and fix that cable) but that possibility is always there. So as far as the P (partition-tolerance), this is something that&amp;rsquo;s not optional if it&amp;rsquo;s a distributed system. So the layman definition of the CAP theorem is: between consistency, availability and partition-tolerance, you can only have two of them. But really, what it actually is, is that if you have a distributed system, you have to choose between consistency or availability. Now, not to belabour the P part but P really is the keystone of all distributed system problems. If it weren&amp;rsquo;t for the possibility of partitions, creating a consistent and available distributed database would actually be trivial. For concept of such painful importance, you would think it would deserve some fancy Greek letter like ∏, but now it&amp;rsquo;s just P. Now I sort of glossed over consistency and availability, I&amp;rsquo;ll try and explain using just a quick little story about why you can only have one of the two. So imagine you have three men that are sitting at a bar and the bartender gives each of the men a whiskey, then if you walk up to any of the man and ask what was your last drink, they would of course say whiskey. Now let&amp;rsquo;s say one of the three men gets up to go to the bathroom. In distributed system&amp;rsquo;s speak, he&amp;rsquo;s been partitioned from the group. He can&amp;rsquo;t communicate with them because he&amp;rsquo;s in the hallway walking to the bathroom. Now while he&amp;rsquo;s gone, walking away, the bartender gives the two men that are still sitting at the bar a beer but now we have a problem because depending on who I ask, either a man at the bar or the one on the way to the bathroom, I&amp;rsquo;ll get a different answer to the question of what was your most recent drink. So what this means is the three men would give inconsistent answers because as a system what you want is a system where no matter who you ask, you always get the same answer. That&amp;rsquo;s what consistency means. But the man on his way to the bathroom could say I&amp;rsquo;m a minority here, I can&amp;rsquo;t reach consensus with my friends so I&amp;rsquo;ll just refuse to answer. So in other words, he&amp;rsquo;s now unavailable. He&amp;rsquo;s unavailable to answer your question. Not that he&amp;rsquo;s down or he&amp;rsquo;s crashed or anything. It&amp;rsquo;s a conscious decision on his part to not answer that question because that way he&amp;rsquo;s not inconsistent: he&amp;rsquo;s just not answering and that&amp;rsquo;s the decision that must be made. You can either be consistent but unavailable, or you can be available but inconsistent in the face of a partition, but you can&amp;rsquo;t be both at the same. So that&amp;rsquo;s sort of my parable of the CAP theorem and on why you have to pick consistency or availability but you can&amp;rsquo;t have both.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: So using your analogy, if that man chooses to be available, then his answer may not be correct.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Right, it could be inconsistent because the last drink he had was whiskey. But (for) the other guys, the last drink they had was beer so they would give you inconsistent answers if you ask the wrong one. The problem is you never know which one you&amp;rsquo;re asking. Now remember I&amp;rsquo;ve said that the man can decide that he&amp;rsquo;s a minority and can&amp;rsquo;t reach consensus so he&amp;rsquo;ll just refuse to answer. This is actually how some databases like Mongo work: they will vote on a master. The master is the one that gets to communicate. But a master can only be elected to be the one to answer questions if it&amp;rsquo;s in a majority. So if a network partition occurs, the minority has just lost the ability to be elected. In the CAP theorem sense, hypothetically Mongo could remain consistent. I&amp;rsquo;m not going to speak to whether it will remain consistent because this is a technical issue and working for a competitor I probably shouldn&amp;rsquo;t exactly go into any sort of rant about whether it&amp;rsquo;s consistent but it could be.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK. But it can&amp;rsquo;t be both consistent and available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Right. That said, that doesn&amp;rsquo;t mean that a system can&amp;rsquo;t be neither. It could be neither consistent nor available and there are databases that are like that.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: What&amp;rsquo;s the point for that?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Well, there are ones that were designed this way. &lt;a href="http://www.mpi-sws.org/~druschel/courses/ds/papers/cooper-pnuts.pdf"&gt;PNUTS&lt;/a&gt; is one of the more famous examples and they do it for lower latency because they don&amp;rsquo;t actually need either. They just want to be as fast as possible. But there are others that are neither consistent nor available just by broken design basically. These are really hard problems and it&amp;rsquo;s one of those things. We&amp;rsquo;re talking about edge cases so it really depends on the size of your clusters. Some people will run three machines and they&amp;rsquo;ll be fine most of the time. I actually spoke with a potential customer a few weeks ago. When I even said what if one of your current setup breaks and they said, well we&amp;rsquo;ll be down for a minute and we&amp;rsquo;ll fire up another one and we&amp;rsquo;ll be fine. That was acceptable to them. That was not acceptable to everybody: some people can&amp;rsquo;t be down for even a minute. So what I find a lot of times in these cases where you have some of these databases that aren&amp;rsquo;t as consistent as they lead on to be. People never in practice really run into them anyway because they are just not that big. Now, when you start talking about dozens of nodes or hundreds of nodes, that becomes a different case: failure is no longer an edge case. Failure is just common because if you think about the fact that if you have some small percentage, say even 1% chance that one of your nodes goes down and you have 100 nodes, there&amp;rsquo;s pretty good odds that one of those nodes is going to be down at any given time. So different systems, like Riak, is designed with that sort of high availability in mind. Now of course, what it gives up is consistency. It&amp;rsquo;s an &lt;a href="http://en.wikipedia.org/wiki/Eventual_consistency"&gt;eventually consistent&lt;/a&gt; system but it&amp;rsquo;s not a fully consistent system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK. When you talk about availability and consistency, it seems like most databases will optimize for one of them. Is it like a continuum between the two?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: It&amp;rsquo;s very much a continuum and it&amp;rsquo;s not just a continuum between consistency and availability for design. In some cases, it&amp;rsquo;s how you configure your network or how you configure your servers. HBase, for example, you can make it either a consistent or available system. Something like Riak, in theory, could be written in such a way that you could have a fully consistent but not available system. &lt;a href="http://cassandra.apache.org/"&gt;Cassadra&lt;/a&gt; could be possibly be written that way too. The reason it&amp;rsquo;s not done that way is because people just don&amp;rsquo;t want it. We kind of have different customers. People use different databases for different reasons generally speaking, and to answer your question, they do tend to lean one way or the other. But whether they succeed is kind of a different question.&lt;/p&gt;

&lt;h4&gt;Part 2: Relational Databases&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK. I think this is a good time for us to dive into different genres of databases and see where they fall into this theorem and also what are the trade-offs that they made. Or in other words, when we pick those databases, why do you want to choose one of the databases? We&amp;rsquo;ll start with relational database cause that&amp;rsquo;s been around since forever. Where do you think relational databases fall into this CAP theorem?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Well, the CAP theorem only applies to distributed systems generally. It&amp;rsquo;s one of those thing that technically applies to non-distributed systems because if you give up partition-tolerance, you&amp;rsquo;ll have consistency and availability, but who cares? At that point, it&amp;rsquo;s like why are you even discussing an inherent problem of distributed systems in a non-distributed sense.
We can talk about Postgres for example. In any non-distributed systems, you have &lt;a href="http://en.wikipedia.org/wiki/ACID"&gt;ACID&lt;/a&gt; compliance. It&amp;rsquo;s Atomic, Consistent (completely different meaning than the CAP theorem of consistent by the way, which makes it very confusing to talk about), Isolated and Durable. These are what you want out of a transaction. It either succeeds or it doesn&amp;rsquo;t and entirely linear isolability is another term for it. Now, when you distribute them. This is what people don&amp;rsquo;t necessarily think about. If you have a backup of your relational database (which you should), you have effectively created a distributed system unwittingly and unknowingly. If you&amp;rsquo;ve ever had a relational database that has a backup and then your primary database crashes, you have to load from the backup and you&amp;rsquo;ve lost data. You have effectively lost consistency in the CAP theory sense. You&amp;rsquo;ve also lost availability because you were down for a while when you were trying to update. I&amp;rsquo;m not sure if that technically counts as being unavailable because your entire system is down then so there&amp;rsquo;s nothing up to even answer requests. There is another case, when people have relational databases and they don&amp;rsquo;t think about the CAP theorem does apply here, is cache: if you throw &lt;a href="http://memcached.org/"&gt;Memcached&lt;/a&gt; in the front of your relational database to more quickly answer requests, you again run into the CAP theorem: you have a consistency problem. Cache expiry is an eventually consistent issue. It&amp;rsquo;s a tough thing to avoid. Simply claiming that I have one big server and I don&amp;rsquo;t have to worry about these issues is completely missing the point. That&amp;rsquo;s of course still assuming only one server (but you can distribute relational databases, of course master/slave applications really come and set up). Again, this is one of those cases where you&amp;rsquo;re making some sort of consistency-availability trade-off. Very often it&amp;rsquo;s still eventually consistent. You can lock and say okay we want full consistency but now you have lost your availability. When I say lock, I mean lock across the network, not just lock within a single database because that&amp;rsquo;s effectively what you need: you need some sort of lock for full consistency. You need a consensus between your nodes. Usually what you&amp;rsquo;re giving up at that time is latency. You&amp;rsquo;re increasing your latency because you have to wait for all of these responses to happen.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: But latency is not part of the CAP trade-off.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: It is not. This is actually something that has been pointed out several times. &lt;a href="http://www.cs.berkeley.edu/~brewer/"&gt;Eric Brewer&lt;/a&gt; has mentioned this fact as well. Like I said before about PNUTS for example, they gave up consistency and availability for lower latency.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: You talked about master/slave and replication type of scenario. What about sharding relational database? I know it&amp;rsquo;s kind of &amp;hellip; You can also do that, but it comes with trade-off.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: It does come with trade-off. When people talk about distributed, that&amp;rsquo;s sort of a catch-all term for just having multiple computers. But when you&amp;rsquo;re talking about databases, there&amp;rsquo;s generally only two reasons you distribute: either you&amp;rsquo;re looking to increase availability by just being up more often and being safe if a node crashes (redundancy) or if you&amp;rsquo;re looking to expand resources so rather than just having one DB server, you horizontally scale. When you split one table to live on multiple servers, that&amp;rsquo;s sharding, that&amp;rsquo;s expanding the resources, whether it&amp;rsquo;s disk or CPU or RAM or whatever. You&amp;rsquo;re expanding the resources horizontally across multiple servers. Although that&amp;rsquo;s a good thing, unfortunately you&amp;rsquo;ve introduced an element of danger now because remember when I mentioned earlier that the more servers you add, the odds of any one of those going down remains the same so for each server, out of the entire group, it&amp;rsquo;s a high chance that at least one of them will be down. If you have 1000 nodes, one of them will probably go down. But the problem is, if you only have one copy, you have corrupted your data because say you&amp;rsquo;ve divided your table across 10 different servers and one of them goes down, you&amp;rsquo;ve lost 1/10 of your data. So you deal with that by also replicating and that&amp;rsquo;s the redundancy; that&amp;rsquo;s the safety part. But also, it&amp;rsquo;s good for reads as well: you can read from multiple servers so the resource requirements of any individual one has gone down because presumably you are dividing your reigns across all of these redundant servers as well. That of course introduces the problems of having to choose between consistency and availability though because once you&amp;rsquo;ve replicated a value to multiple servers, you&amp;rsquo;ll have to keep them all in sync: you&amp;rsquo;ll have to reach consensus. But reaching consensus is a very difficult thing and a lot of times what you pay in is latency. A quick comment, I keep throwing this word consensus around. The idea of consensus is in two parts: consensus is safely and liveness. Safety is the promise that nothing bad will happen that liveness is the promise that something good will eventually happen. There&amp;rsquo;s an old paper from the 80s called the &lt;a href="http://www.cs.yale.edu/homes/arvind/cs425/doc/fischer.pdf"&gt;FLP Impossibility Proof&lt;/a&gt; that shows you can&amp;rsquo;t actually guarantee both full safety and full liveness in a distributed system. Before this, people just threw out &lt;a href="http://en.wikipedia.org/wiki/Two-phase_commit_protocol"&gt;two-phase commit&lt;/a&gt;, thinking that it will solve their consensus problems. But if a network partition occurs, again as I said partition is the crux of all distributed system problems, you can lose liveness and actually you can even lose safety. So then they created &lt;a href="http://en.wikipedia.org/wiki/Three-phase_commit_protocol"&gt;three-phase commit&lt;/a&gt;. It has better liveness guarantees but it&amp;rsquo;s still potentially unsafe in the face of network partition. That&amp;rsquo;s why, maybe it&amp;rsquo;s been 10 years or more, &lt;a href="http://en.wikipedia.org/wiki/Leslie_Lamport"&gt;Leslie Lamport&lt;/a&gt; created a consensus system algorithm called &lt;a href="http://en.wikipedia.org/wiki/Paxos_algorithm"&gt;Paxos&lt;/a&gt; and it&amp;rsquo;s very powerful and it&amp;rsquo;s very flexible and it&amp;rsquo;s also very, very complex. There are very few good implementations out there. The importance of these consensus algorithms are unless you&amp;rsquo;re routing all of your traffic through one main master server, you can&amp;rsquo;t really have consistency in your distributed system, not in the way that we want liveness in terms of availability. What we live with is eventual consistency, which itself is a liveness (in terms of availability) property. &lt;a href="http://www.bailis.org/"&gt;Peter Bailis&lt;/a&gt; came up with this concept called &lt;a href="http://pbs.cs.berkeley.edu/"&gt;PBS (Probabilistically Bounded Staleness)&lt;/a&gt; and the point of it is to answer two questions about eventual consistency: first question is how eventual is eventual consistency and the second one is how consistent is eventual consistency. Again this is important because, if you&amp;rsquo;ve noticed, the very first word in there is probabilistically: what&amp;rsquo;s the probability of you values being stale because that&amp;rsquo;s really what it comes down to in most of these distributed systems. It&amp;rsquo;s when you have an eventually consistent system, you&amp;rsquo;re talking about probabilities now. This just cuts right through the heart of distributed databases because you want to be as consistent as possible but you also want to be as available and with the lowest latency, and to get all of those things is just not in the cards: you&amp;rsquo;re giving up something at some point. So the best thing to do is just figure out what exactly it is you need from a business case and make a decision that way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: When we do get to the specific parts, we&amp;rsquo;ll ask you to give us some examples of use cases on how to choose. So we&amp;rsquo;re still on relational database?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Oh yes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: I kind of want to ask the question of open-source database like &lt;a href="http://www.mysql.com/"&gt;MySQL&lt;/a&gt; and Postgres v.s. proprietary database such as &lt;a href="http://www.oracle.com/us/products/database/overview/index.html"&gt;Oracle&lt;/a&gt;, &lt;a href="http://www-01.ibm.com/software/data/db2/"&gt;DB2&lt;/a&gt;, &lt;a href="http://www.sybase.ca/products/databasemanagement"&gt;Sybase&lt;/a&gt; and so on. Is anybody there still using non-open-source databases? Is there a reason not to use open-source databases?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Not really. That&amp;rsquo;s about as boring and straight-forward as it can be: there&amp;rsquo;s no good reason not to use an open-source database. There once was but these aren&amp;rsquo;t true anymore. The fear of vendor lock-in is pretty well abated. It&amp;rsquo;s not problem in the way that it was. If you use MySQL and you don&amp;rsquo;t like it, try Postgres. If you don&amp;rsquo;t like that, try &lt;a href="http://voltdb.com/"&gt;Volt&lt;/a&gt;. There are so many relational databases out there that are open-source. You may as well just go for it and use them. I have been in charge of three conversions to MySQL or Postgres from a proprietary database. Two of them were Oracle and one was &lt;a href="http://www.microsoft.com/en-us/server-cloud/products/sql-server/"&gt;SQL server&lt;/a&gt;. In every case, we saw a significant improvement in performance largely around just because as we were doing this port we&amp;rsquo;ve been able to refactor everything but the point being that it was successful, it worked well and it was cheap. There&amp;rsquo;s often a fear of using open-source systems as who&amp;rsquo;s going to support this. I can&amp;rsquo;t think of a single open-source database that doesn&amp;rsquo;t have a company backing it right now. They all do. Maybe Couch doesn&amp;rsquo;t, I don&amp;rsquo;t know. There&amp;rsquo;s &lt;a href="http://www.couchbase.com/"&gt;Couchbase&lt;/a&gt; but that&amp;rsquo;s kind of its own database. They all do. There&amp;rsquo;s always somebody to call if you have problems. They all have books; they all have communities. You can always just email &lt;a href="http://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;. The reason I bring that up is the fear of using an open-source database I find is generally the fear of the lack of support like who do I call if something goes wrong. However, I&amp;rsquo;ve had problems with industrial commercial support as well. If it means that much to you to have someone in the blind to be paid $500,000 a year in licensing fee then you have more money to throw around than I do because I would rather just hire five engineers for that price or unless if you&amp;rsquo;re in Silicon Valley then two engineers and just have them on the job.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: So out of the open-source relational databases, I know you have experience with Postgres so that&amp;rsquo;s sort of front-runner nowadays.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: That&amp;rsquo;s my front-runner. I&amp;rsquo;ll be honest, in the past three or four years, I haven&amp;rsquo;t really been involved much in the relational database world. I pretty much spent my lifetime (10,000 hours) in front of my computer on relational databases and it&amp;rsquo;s not something I&amp;rsquo;m interested in going back to. I still think they&amp;rsquo;re amazing and I still think relational databases will solve the majority of people&amp;rsquo;s problems. I think that far too many people rush to alternatives for no good reason. But I definitely still throw out Postgres because I just prefer the design and now again this is my entire biased opinion. It&amp;rsquo;s the one that I&amp;rsquo;m the most comfortable with. It&amp;rsquo;s the one that most of my knowledge of the internals peaked around 2004 and at the time its internals were far cleaner and much more flexible than something like MySQL at the time: you could actually write your own indexes rather painlessly. They have just implemented it so you could write your own B-tree style indexes with even less trouble than just flexible scripting almost. I just enjoyed the flexibility and the community.&lt;/p&gt;

&lt;h4&gt;Part 3: NoSQL databases&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Let&amp;rsquo;s move away from relational databases. There&amp;rsquo;s this &lt;a href="http://en.wikipedia.org/wiki/NoSQL"&gt;NoSQL&lt;/a&gt; basket for all of the non-relational ones. My first question is why did NoSQL emerge in the last 10 years? What has changed to give birth to this set of databases?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: My theory is that there are really two reasons why NoSQL emerged: one is just the fact that as more and more people have got on the web, more and more people have constantly connected devices, we&amp;rsquo;re just collecting more data and that amount of data was something that was never envisioned for relational databases. Relational databases can handle lots of data. They can go into TB range but it&amp;rsquo;s one thing to be able to collect TBs of data and it&amp;rsquo;s another thing to be able to respond quickly to requests once you&amp;rsquo;ve collected that data. So one is just the amount of data and two is that over the years, relational databases have been very specifically tuned for business intelligence: for doing analytics and data warehousing and things like that. Because they weren&amp;rsquo;t dealing with such massive data sets most of the time and if they were it was some sort of just data that you could just put in the background and just archive it and then build a data warehouse star pattern (schema). I think those two reasons, more than any other, are responsible for companies having to effectively go along. The first few NoSQL databases that popped up were Amazon and Google because databases like Oracle just couldn&amp;rsquo;t keep up. They needed something different. As more and more people started having these same problems, at the same time, virtualization was getting easier and cheaper to scale out horizontally. This is something that relational databases historically have been very bad at so they took advantage of all these virtualized systems and were able to scale out. So these databases were designed to do that. HBase, &lt;a href="http://en.wikipedia.org/wiki/Dynamo_(storage_system)%E2%80%8E"&gt;Dynamo&lt;/a&gt; were designed to do this. That&amp;rsquo;s sort of my pet theory on why NoSQL has emerged recently. But you&amp;rsquo;re starting to see in some cases relational (databases) try to catch up. I&amp;rsquo;ve mentioned VoltDB. It&amp;rsquo;s made to be a horizontally-scalable database. I don&amp;rsquo;t know anyone that uses it. It may well be great. &lt;a href="https://voltdb.com/about/leadership/michael-stonebraker/"&gt;Stonebraker&lt;/a&gt; is brilliant so I would definitely never bet against him. But I don&amp;rsquo;t know if people have just decided that they don&amp;rsquo;t need all of the relational trappings or if they believe that they can&amp;rsquo;t have them. But for some reason, people have been flocking to NoSQL database world.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dingding&lt;/strong&gt;: So, you can build a distributed relational database with distributed nodes so you can have hundred of Postgres nodes and just build an upper layer to make it distributed. What&amp;rsquo;s your opinion of that against a NoSQL solution?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: It&amp;rsquo;s interesting. If you think of that architecture and you work with the architecture of something like Mongo, you&amp;rsquo;ll see a lot of similarities where you have a mater node that replicates and then you&amp;rsquo;ll have your Mongo server that accesses a router and your configuration that track where different keys go, they&amp;rsquo;ll feed Mongo to decide this shard goes to this replica set. Very similar to the way you would do manually in a relational database. But again your problem comes down to the CAP theorem. It comes down to the fact that you still have to make a consistency-availability trade-off. You can use something like Prosgres but you don&amp;rsquo;t really get all the benefits anymore. You can&amp;rsquo;t really easily or realistically join, for example, across a network. Really, the power of relational databases come down to the fact that you can normalize all of your data structures and if it&amp;rsquo;s normalized, you can query it in pretty much any way you want and more specifically joining. You can join values and create new table types and relations that you get as response. You have effectively lost the ability to do both of those once you start distributing your relational database unless you&amp;rsquo;re just doing straight-up replication. I know people do this a lot but sometimes I get the feeling they do it just because they just don&amp;rsquo;t know any better. If you&amp;rsquo;re doing that, I would recommend just trying something else. Just try a system that was designed to be distributed from the ground up and not shoehorn a system that was not designed to be distributed into becoming a distrusted system. The important thing to note about all of the databases that we&amp;rsquo;ve mentioned so far, with enough code, you can make them all do anything if you want to hack enough around it. The question is how much effort you really want to put into designing your own kind of database ecosystem, which effectively what you&amp;rsquo;re doing. You also have to think about operational costs. Yes, you can create these sort of custom clusters of relational database to do the things you want but now you effective have to worry about is this a master node or is this a secondary or is this a configuration or is this a router. You&amp;rsquo;ve sort of offloaded your development costs and put it squarely on your operational costs. This is something developers don&amp;rsquo;t often think about when they are coding: coding is the cheapest and fastest phase of your process. The more expensive and much long part of it is operations. Assuming you&amp;rsquo;re successful as a developer, someone is going to be running and maintaining that code for years. If you design it in such a way that is complex to scale and maintain, you&amp;rsquo;ve effectively just given some future person a whole lot of work. If you&amp;rsquo;re not distributing, just use relational database. They&amp;rsquo;re easy and they have a ton of research behind them for years. SQL is an amazing language. It&amp;rsquo;s the most successful language ever. Think of any language that&amp;rsquo;s been around that long. You&amp;rsquo;re talking about C and SQL in some form, obviously not in the SQL specification. That&amp;rsquo;s much older than 40 years but the whole relational concept. If you&amp;rsquo;re distributing, go after a NoSQL solution.&lt;/p&gt;

&lt;h4&gt;Part 4: Document Databases and Column Databaes&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Talking about NoSQL, a lot of people equate them to document databases. That seems to be most popular one nowadays. So why don&amp;rsquo;t we start with document databases? What are the trade-offs and maybe in the CAP theorem context?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Again, I wouldn&amp;rsquo;t necessarily want to frame this as a CAP theorem thing because there&amp;rsquo;s no reason we couldn&amp;rsquo;t design a document database that&amp;rsquo;s a consistent v.s available. If you look at Mongo for example. It&amp;rsquo;s designed to be a consistent database. One thing I will say is that &lt;a href="http://aphyr.com/"&gt;Kyle Kingsbury&lt;/a&gt; recently wrote &lt;a href="http://aphyr.com/tags/jepsen"&gt;a series of blog posts&lt;/a&gt; about many of the databases about many of the databases that we&amp;rsquo;re talking about here where he stress-tests and he creates artificial partitions and queries them in different scenarios and looks at the failure rates. What he uncovered is that a lot of the claims that are made by some of these implementations don&amp;rsquo;t necessarily hold up in a very tough scenario. Again, fairness, being what it is, in the real world, these may not come up quite as often. He really hammed it. One of the losers in his initial attempt was the default settings that come with Riak which was kind of painful for some of us at Basho to have to see but there are settings you can make it better and most of the databases did. There were configurations that you could follow. I will try to find that link and send it to you because they&amp;rsquo;re definitely a fascinating read. As far as document datastores, the two that I covered in my book are Mongo and Couch. Mongo, for many people, is synonymous with NoSQL: if you say NoSQL, it&amp;rsquo;s the first thing that pops into their head. There&amp;rsquo;s a lot of reasons for this, one of which is it&amp;rsquo;s the first popular one to come out and it still remains the most popular as far as just sheer number of users. Mongo, in the CAP theorem senses, is slated to be consistent and of course that means it&amp;rsquo;s beat is availability, in the technical sense, if there&amp;rsquo;s a partition. Although that used to be a selling point, Mongo actually is from the middle of the word humongous because it was supposed to deal with humongous data sets. By design it&amp;rsquo;s very similar to what you would do manually if you took a relational database and wanted to create a master/slave setup yourself and shard keys and things like this. Part of the popularity is just the fact that it&amp;rsquo;s really easy to use if you are used to a relational system. It comes with a very simple query language where you just put values in. (When) I say document, I don&amp;rsquo;t mean a document like a PDF or Word file or anything like that. A document, in this case, is just JSON and is keyed by an id. Other than that, you can query by any value. This is actually really important if you are designing something and you&amp;rsquo;re not entirely sure where you&amp;rsquo;re going with it because unlike a relational database where you have a very structured schema and you have to say I&amp;rsquo;m going to have a person table and a person is going to have a first name and a last name then later on you decide oh I want to add middle name too. Now you&amp;rsquo;ve got to create a new column called middle name and you&amp;rsquo;ve got to actually tell the database what that schema is. In a document data store, like Mongo and Couch, you don&amp;rsquo;t actually have to tell it anything in advance: you just give it a key-value, just like any JSON, &lt;code&gt;first:fist_name&lt;/code&gt; and &lt;code&gt;last:last_name&lt;/code&gt;. And then if you just decide you want to start adding middle name or initial, you just start adding middle name fields. You don&amp;rsquo;t have to migrate your old data in any way. This obviously has a cost on retrieval: it&amp;rsquo;s easy to put values in but when you pull values out, you code has to deal with the fact that there might be nothing there. So over time, your code could get crufty. But if you&amp;rsquo;re going very fast, this is a trade-off that people, especially developers, don&amp;rsquo;t mind paying. Couch has the same benefit. The difference being though the way you query Couch is you actually write MapReduce and that MapReduce is a view over your data in some way. For example, if you want to be able to query by last names, you would write a MapReduce that would extract last names out of your values and any time you would do a search, you search for values that have been extracted by this MapReduce view. Mongo doesn&amp;rsquo;t require you to do that: for its query mechanism, it&amp;rsquo;s much more like a relational database where you&amp;rsquo;ll just say give me very last name that match this name or give me everything between this last name range. It feels much more like a relational database. That said, if not a relational database, you don&amp;rsquo;t normalize it. It&amp;rsquo;s almost inherently denormalized. Since it&amp;rsquo;s JSON, you can nest values: your person could contain an array of pets. So in relational databases you might have to a separate pets table and then join it with a person. In this case, your person will just contain pets. That makes it really easy to build and it makes it really easy to put data in. It makes it slightly more difficult to get data out because if you just for example want the names of everyone&amp;rsquo;s pets, you&amp;rsquo;ll have to do a more complex query than you would in a relational database where you just say &lt;code&gt;select * from pets&lt;/code&gt;. You would have to say, get all the users, find all the pets, extract those names and then give them to me as a list or as a collection. That adds a little more complexity when you write your queries. This is something you&amp;rsquo;ll find with most NoSQL databases: it&amp;rsquo;s much easier to get data in than it is to get data out. And this is part of the reason in my opinion you&amp;rsquo;re seeing popularity of other third-party analytics tools like &lt;a href="http://hadoop.apache.org/"&gt;Hadoop&lt;/a&gt;, which is like a MapReduce engine where it can perform queries across the system predicated on the idea that when you have TBs of data spread across multiple servers. It&amp;rsquo;s cheaper to take the algorithm and just send it to the servers, let them compute the value and then give it to you than what you would do in a relational side where you stream data and then compute it because it doesn&amp;rsquo;t matter because it&amp;rsquo;s all on the same box anyway.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: So it&amp;rsquo;s easier to ship your queries or the algorithm themselves to the data because you have a distributed system than to getting pieces of data from each place and try to join them together or get a result.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Yes, it&amp;rsquo;s just the matter of reducing the amount of data going over the wire. When you&amp;rsquo;re dealing with big data, you very often have to invert your way of thinking and this is one of those cases where you invert the order of operations you are normally used to doing. One of those inversions I mentioned before where very often you don&amp;rsquo;t necessarily think about how you&amp;rsquo;re querying the data. You just put it in there and you worry about querying it later. That obviously has its own cost. You&amp;rsquo;re going to be paying these costs anyway. In a relational database, you just pay them in a very different way: you pay them upfront, in the design phase. While you&amp;rsquo;re sitting down designing something, how many times have you done an application that you&amp;rsquo;re using a relational database and you start with a whiteboard and you start drawing tables and saying okay we&amp;rsquo;re going to want to do this and this is what our scheme is going to look like and you start coding and say oh crap, I need a different table or I need join tables to sit in-between these because the join needs to actually have another value hanging off of it, so I need to create a whole new table. These design decisions are very trivial when you&amp;rsquo;re dealing with a lot of these NoSQL solutions but the querying is more difficult.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Yeah, I guess the compliment with relation databases was querying is really powerful, really super easy. Whereas when you would start to add data, then you start to face the CAP theorems. I still want to talk a little bit more about Mongo because that seems to be the popular one. We talked about availability and some of the trade-offs but from a data modelling point of view, if you just compare relational database with a Mongo data, I feel like Mongo is a very opinionated way. It&amp;rsquo;s like if you know exactly you want to traverse the data, then Mongo is perfect. In your example, a person and cat. Maybe cat has toys. If you know you always go from person to cat to get toys, you never would just aggregate some toys that way, then it&amp;rsquo;s perfect. You can always go that way. But you&amp;rsquo;re losing the flexibility. You may not anticipate you do need to do another scan.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Yes. The flexibility you lose is the query flexibility of a relational database. That said, no database has the query flexibility of a relational database. The relational database designed SQL is a query language. It&amp;rsquo;s a declarative language. It&amp;rsquo;s structured. But that&amp;rsquo;s exactly its strength and largely that&amp;rsquo;s its biggest strength. And relational databases are the only ones where you have a somewhat structured schema. Column-oriented databases like HBase, you define your schema upfront. Cassadra is another: it&amp;rsquo;s topologically Dynamo-based like Riak is but its data structure is column-oriented like HBase is where you define what your column families are. They do have a little more flexibility than a relational database. The problem for relational databases is you put one value per row for example whereas in a column-oriented data store you can have as many individual discrete values as you want without adding rows: you&amp;rsquo;re just adding that value because data is stored in columns rather than rows. A simple example would be a wiki where the key might be the title of the wiki page and you might have multiple revisions. In a relational database, unless you denormalized it, if you just said okay my page table will have one column for the title and one column for the contents of the page. What you&amp;rsquo;ll find is the title never changes, so you&amp;rsquo;ll just be replicating that a lot and the contents of the page change quite often. Whereas in a column data store, you would have just one column family page and the title would never change. That would be one column and another column would be the contents of the page and it would change. When you do a query, it&amp;rsquo;s a row but it&amp;rsquo;s almost like a pseudo row. You&amp;rsquo;re just saying okay give me the most recent title and the most recent page contents. You actually can get a lot out of this: you can give them a time to live, which is very nice. There&amp;rsquo;s a reason that Facebook&amp;rsquo;s messaging system runs on HBase. The ability for messages to have a time to live is something I presume they are able to leverage and also it scales up crazy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Right. I guess if you do this on a SQL database, it&amp;rsquo;ll be non-trivial or you&amp;rsquo;ll have to write a lot of code to run it like that.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Yes, or you&amp;rsquo;ll have to have some sort of custom extension. I wouldn&amp;rsquo;t be surprise if something like Postgres had a time to live extension or a timestamp. Generally what you&amp;rsquo;ll do is you&amp;rsquo;ll just timestamp a value and then as part of query you&amp;rsquo;ll just say give me this range and then maybe manually delete everything that&amp;rsquo;s outside of that range. But that would be one trivial way of doing it but it&amp;rsquo;s nice to have these things built-in, that&amp;rsquo;s for sure. Again, like I said before, most of these databases, with enough code, you can make them do them to anything but how much code do you want to write?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dingding&lt;/strong&gt;: When we talked about document DB, you mentioned some column DB like HBase and Cassadra. Document DB to me is similar to column DB but with few limitation and more improvement. So what&amp;rsquo;s your opinion with these two and the difference between them?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Actually a document DB is much closer to a key-value store in this way because Mongo or Couch, whether its queries or views, by default is not actually indexing the values. The only thing that&amp;rsquo;s indexed is the id, which is just a key look-up. But if you query against a column that&amp;rsquo;s not indexed, it&amp;rsquo;s just a full table scan. The same is true with Mango and Couch. The same is not true with something like HBase where these are sparsely ordered. You&amp;rsquo;re actually always scanning effectively. I guess you can do key-value look-ups as well but generally you scan ranges of values. To index, you&amp;rsquo;ll effectively index manually. Again, depending on how much code you want to write, you can index with a key-value store as well. If you have Redis for example, which is a key-value store and you have a deeply nested value and you want to be able to query by certain values, you can just create another key type and say okay I&amp;rsquo;m going to be a quick-up and point to the, for example, say you have a &lt;code&gt;person:social_security_number&lt;/code&gt; and that&amp;rsquo;ll contain all of the person data that we talked about earlier with our Mongo example: searched by last name. You can just create a key like &lt;code&gt;last:last_name&lt;/code&gt; and then it can point to the correct &lt;code&gt;person:social_security_number&lt;/code&gt; and then it&amp;rsquo;s just a look-up. You can do this with a key-value store. In relational databases, obviously to do it effectively, you need some sort of ability to scan. So you could effective write your B-tree if you can&amp;rsquo;t scan. It depends on how much code you want to write but I would say generally speaking, I actually find much more similararity between a key-value store and a document data store than I would in a column-oriented data store and a document data store.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: I would say that as well. I feel like one way you look at document databases, they&amp;rsquo;re also key-value stores but the value can be nested.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Riak, for example, all values are opaque, meaning it doesn&amp;rsquo;t care what the value is so you can actually put JSON as a value inside of Riak. It has a secondary indexing feature so if you want to index against some of those values, you just make it index and you query against that index. That would make it closer to Mongo in that respect as far as querybility. Or if you want to use MapReduce, you can do that too, which case it&amp;rsquo;ll be something more close to Couch although Couch is considerably more efficient on the way that it builds its views cause it pre-builds them and just keeps them updated using partial MapReduce as you make updates. You&amp;rsquo;re absolutely right. There&amp;rsquo;s much more similarity than differences in that respect.&lt;/p&gt;

&lt;h4&gt;Part 5: Key-value Stores&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: We talked about document databases and column databases already and you mentioned Mongo, Couch, HBase and Cassandra. Let&amp;rsquo;s move to key-value stores. We already touched on Redis already. Maybe you can talk about Redis and Riak in comparison? Those two key-value stores. What are the characteristics of each and so on?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Sure. The reason I added two key-value stores in this book was as an example that the network topology can be very different or the use cases can be very different for a similar modelling style. So key-value store is effectively like a hash map or a dictionary or an object (if you&amp;rsquo;re a JavaScript person) where you just have some key, you store a value with that key and if you want that value later, all you require is that key to get the value back out. This is really useful for many things, one of which is caching. That&amp;rsquo;s where Redis excels. Redis lives entirely in main memory so it&amp;rsquo;s very, very fast. It&amp;rsquo;s clever in the fact that it&amp;rsquo;s sort of designed for those caching use cases where you want some sort of intermediary between the way your data might live on a different back-end, whether it&amp;rsquo;s a relational dabase or CouchDB or whatever and the way you might want to use it. It does this by being able to store values that aren&amp;rsquo;t opaque in the way that they are in Memcached. In Memcached, all values are strings. If you do any encoding, it&amp;rsquo;s entirely up to you. You can turn them into lists, you can turn them into objects, hash tables, whatever but you just have to serialize it as a string, store it as a string, pull it back out, de-serialize it and do whatever. Redis does a lot of this work for you. It supports limited data structures like lists, sets. It can do key range searches by wildcard. It also got some other built-in things. &lt;a href="http://redis.io/topics/pubsub"&gt;Pub/Sub&lt;/a&gt; is very popular. It can do interesting things on the data strictures. For example, you can take the value of two keys that might be set and do a set union on them or a set intersection. This is useful in a lot of ways. For example, going back to the person example, you each have pets and toys. Say we each have our own pet and they&amp;rsquo;ll be keyed in some way and they&amp;rsquo;ll contain a set of all of the toys that they play with. My cat might play with a box and a ball of string and your cat might play with a ball of string and feather and we can do a set intersection and say what do our cats agree on and they&amp;rsquo;ll say the set intersection of these two sets are a ball of string. It&amp;rsquo;s the common value. This is something that&amp;rsquo;s fairly unique to Redis in the world of key-value data stores. They are also very common operations when you want cache values. You have a lot more flexibility than you would. Something like Memcached would require you to write code. They would pull the set from my pet, the set from your pet, convert them and then perform the intersection on the client side. Very, very powerful. (It&amp;rsquo;s) part of the reason why its adaption is so high for these use cases. Now, one thing it&amp;rsquo;s not great at is it&amp;rsquo;s not durable. It has durability options but that&amp;rsquo;s really just in the case where a node might crash, it&amp;rsquo;s fast to warm up. As far as being distributed, it&amp;rsquo;s not to the level of many other distributed systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: We talked Redis in the caching context. It&amp;rsquo;s mostly read. I&amp;rsquo;ve seen some people that use Redis as an intermediary to their database so instead of all the operation touching database they can just put in memory and it&amp;rsquo;s fast.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: It&amp;rsquo;s actually faster to write to Redis than it is to read from it. I just experimented once before and I was able to get 100,000 operations writing and 80,000 reading. I don&amp;rsquo;t remember what the time frame was. Maybe a second. Part of it is because it has to convert the value into something when it reads. Actually, I often recommend this a lot. It&amp;rsquo;s for data transformation using Redis as an intermediary. I actually did a sample project: in the last chapter of the Seven Databases book I used Redis as an intermediate data transform from a tab-separated value into a document that&amp;rsquo;s suitable to be stored in Couch. And it&amp;rsquo;s because it&amp;rsquo;s much faster to find duplicates in a Redis than it was to try and do a write to Couch that may have conflicted version-wise. This is a multi-threaded application to go faster. If two writes are going at the same time then Couch would reject the attempt at one of them so it has to go back to the application and say okay Couch rejected this and it has to read Couch again to get the newest revision number and then attempt to write again and sometimes that round-trip will push it back again because another write has succeeded in the interim so you can fill up your write effort rather quickly. So I 100% agree with that as well. It&amp;rsquo;s not just caching. That&amp;rsquo;s just the most common use case. Data transform is fantastic for it. The other key-value store that I wrote about is Riak which we should definitely talk about. Riak is actually designed to be a highly available system. It&amp;rsquo;s made for the case where you can&amp;rsquo;t be down for any amount of time. We&amp;rsquo;ve actually have customers that have had 100% up-time as crazy as that might seem. &lt;a href="http://www.comcast.com/"&gt;Comcast&lt;/a&gt; mostly gave a &lt;a href="http://vimeo.com/54270121"&gt;talk&lt;/a&gt;. They said since they&amp;rsquo;ve installed Riak, for the past three years, that system has been up 100%, which is unheard of in the database world. Part of the reason is because of the way it&amp;rsquo;s designed: it has built-in sharding and replication. You can lose many servers and still be up. One of our customers had 30 nodes, and if I recall correctly, 13 of the 30 nodes went down and they didn&amp;rsquo;t realize it. It took them like a couple days to notice a lot of the servers were down and they spin them back up and everything was fine. This is really Riak sweet spot. It&amp;rsquo;s not made to necessarily be the fastest store or the largest scale. It&amp;rsquo;s really made to just be up all the time and you could survive a nuclear blast and Riak will be fine as long as you&amp;rsquo;ve got &lt;a href="http://docs.basho.com/riakee/latest/cookbooks/Multi-Data-Center-Replication-Architecture/"&gt;Multi-Datacenter Replication&lt;/a&gt; and you&amp;rsquo;ve replicated all your values in multiple datacenters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: So does the replications happen automatically?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Well, Multi-Datacenter Replication is actually the one thing that we charge for. The single datacenter replication is built-in. When you write a value to Riak, it will by default replicate to three nodes. This is tunable, you can set its value to whatever you want. We usually recommend 3: it&amp;rsquo;s usually sufficient for most cases. That means at least 2/3 of that value&amp;rsquo;s nodes could go down and you still have data available. Then there&amp;rsquo;s Multi-Datacenter Replication which means that you have multiple datacenters that themselves replicate through each other to keep themselves in sync by various means. It&amp;rsquo;s very tunable. This is for a lot of reasons, either data locality: so you can have a cluster in the U.S and a cluster in China and you can have data local to your Chinese customers cause they can access it faster v.s other data that&amp;rsquo;s local to your U.S customers or you can have two datacenters and one is just used for backup or whatever other reasons people have for choosing to replicate at multiple datacenters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: By the way, I&amp;rsquo;m just curious, are you aware of the companies using Riak in China?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: I&amp;rsquo;m not. Actually that&amp;rsquo;s definitely something I&amp;rsquo;m very interested in. If anyone&amp;rsquo;s interested in helping spread the word about Riak in China, I&amp;rsquo;d love to hear from them or if anybody that knows of any companies in China that are using Riak, I would love to hear from them. We are a company that was found in the U.S. All of our first customers are in the U.S, then we went to Europe and six months ago we opened an office in Japan so we&amp;rsquo;re slowly spreading internationally. Any way to speed up that trend would be amazing cause we have not really spread to South America as far as I know as well. We may be everywhere. I just haven&amp;rsquo;t heard about them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Is Riak itself an open-source database?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Riak is 95% open-source. Riak itself is entirely open-source and &lt;a href="http://basho.com/riak-cloud-storage/"&gt;Riak CS&lt;/a&gt;, which is like sort of &lt;a href="http://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt;, object storage. You can run Riak CS where CS stands for Cloud Storage which acts like your own personal S3. It has an S3 interface so you can actually use all of your S3 code. There&amp;rsquo;s a lot of regulatory reasons or just financial reasons that people might want to run their own S3 system and that&amp;rsquo;s entirely free. The only thing we charge for is Multi-Datacenter Replication and of course if anybody wants to buy support. That&amp;rsquo;s true for Riak and that&amp;rsquo;s true for every other database I&amp;rsquo;ve mentioned. There&amp;rsquo;s all sorts of companies behind these databses: Mongo has 10gen; Postgres has a consortium of independent contractors; Neo4j has &lt;a href="http://www.neotechnology.com/"&gt;Neo Technology&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: I was just thinking because when it comes to China, a lot of things are big data and it&amp;rsquo;s typical that you walk into a bank and you have 10,000,000 customers or 100,000,000 customers very easily. Coming back to Riak, you talked about Riak automatically replicates the data records, does it hurt its consistency or how does it make that choice?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: As we&amp;rsquo;ve mentioned previously about the CAP theorem being kind of a spectrum: consistency and availability are in some ways tunable. It doesn&amp;rsquo;t have to be fixed. This comes back to what I&amp;rsquo;ve mentioned about Peter Bailiss and PBS (Probabilistically Bounded Staleness, how eventual is eventual consistency and how consistent is eventual consistency), you can tune your eventual consistency. You can tune your availability and your consistency and in some ways your latency as well is effectively this way. The way Riak does it is it has three values called N, R and W. N is the number of nodes you will replicate a value to eventually. By default, it&amp;rsquo;s 3. W is the number of nodes that will return a successful affirmative before you return back to the client and say yes this write woks so what happens is your write to one of the Riak notes will coordinate the replication. So if you set W to 2, what it&amp;rsquo;ll do is even though eventually all three nodes will have the value replicated that you want, it will only wait until two of them returned a success before it says the write is successful. R is the last one, which is the same thing but for reads. You&amp;rsquo;ll attempt to read from all three nodes but if you set R to 2, only two of them need to return a success for value to get a result. You&amp;rsquo;re not going to wait for all three. Now if you want to make a more consistent system, the consistency can be either write consistent or read consistent. So for example, say that I have my end value and I want to be certain that my write has been replicated to all three so I can set W to 3 and say don&amp;rsquo;t even return until every value has been replicated. At this point, you can be pretty certain that successive reads are going to contain that value but you&amp;rsquo;ve of course slowed down. You paid in latency and availability because if one of those three replicas is down, your write will fail because what you told it is I need all three to work. One of them is down, you only two up, it can fail. That&amp;rsquo;s what&amp;rsquo;s called the quorum. The quorum means if your R(Read) value + your W(Write) value &amp;gt; number of nodes, then you&amp;rsquo;re probably going to be consistent, assuming everything goes according to plan because there&amp;rsquo;s going to be an overlap. It&amp;rsquo;s an easy thought experiment. Say you have nodes A, B and C, and you write successfully to nodes A and C and you read successfully from nodes B and C, then even though only C has the most recent value, you&amp;rsquo;ve at least got the most recent value and you know this. There are details here. I&amp;rsquo;m not going to all of the problems why that&amp;rsquo;s not exactly truly consistent, not in the linearizable sense but it&amp;rsquo;s consistent enough for being a highly available system, which is the whole point. You can flip it over: if you want to write quickly but you don&amp;rsquo;t care to wait, you can set W to 1 and say just write to one node, I don&amp;rsquo;t care which one, and then return. Your latency would go down because you&amp;rsquo;re not waiting for all three replicas to return: whenever the fastest won the return race, then you&amp;rsquo;re just back and you say okay I won. Then when you do a read, if you want the most recent value, you can then set R to the number of nodes and you&amp;rsquo;re effectively reading from all of them cause you&amp;rsquo;re saying at least one of these is going to have the most recent write. But you&amp;rsquo;ve slowed down your read at that time and again you&amp;rsquo;ve paid in latency and if one of those nodes is down, you&amp;rsquo;ve paid in availability. I&amp;rsquo;m oversimplifying this because there actually is a little more detail to it than that: there&amp;rsquo;s durable writes; there&amp;rsquo;s primary writes, primary reads. Because Riak by default does something, and this is something Dynamo does as well called sloppy quorum where if a node you would normally write to or read from isn&amp;rsquo;t available, it will then go to the secondary node, which is the next from the list. It&amp;rsquo;s not a strict quorum in the fact that it would fail. It would actually try to do that right any way. And it would just elect another node to act as a temporary storage. This is like, if you&amp;rsquo;ve gone on vacation and while you&amp;rsquo;re on vacation, you have your neighbour collect your all your mails and then once you&amp;rsquo;ve come back from vacation, your neighbour hands you all your mail. In Riak, this is called the hinted handoff. So once the node has rejoined because it had crashed or because there was a network partion, all of that data gets given back to the primary node, the one that should had it all along. It&amp;rsquo;s tricks like this that allowed Riak to be highly available system. This is why you get these kinds of crazy up-times that you don&amp;rsquo;t necessarily get with all other databases that choose consistency over this kind of availability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: I guess the use cases for Riak is if the business requirement is such that availability is paramount. No way this can be down, money is at stake or &amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Yes, and that&amp;rsquo;s generally the way I would pitch it: if being down costs you something, whether it&amp;rsquo;s money or users, if people would just get tired and leave. This is actually why Amazon designed Dynamo. So Riak is based off the Amazon Dynamo database design as is Cassandra topologically, whereas HBase is designed off Google &lt;a href="http://en.wikipedia.org/wiki/BigTable"&gt;BigTable&lt;/a&gt;. Amazon designed it in this way because they had done research that found that, they had it down to a dollar value, that was for every millisecond of latency, it actually costs this many dollars because people would just get fed up and leave because they do so many transactions so if you&amp;rsquo;re doing a lot of these transactions, you can&amp;rsquo;t have latency, you can&amp;rsquo;t be down. It&amp;rsquo;s interesting because EC2 and S3 famously go down like once a year and since half of the internet is built on EC2, everybody suffers these consequences and everybody freaks out but what&amp;rsquo;s interesting is: Amazon itself doesn&amp;rsquo;t go down if you ever noticed this. A lot of it is because of their internal architecture, not the one that they give everyone else but their own.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: So DynamoDB is hosted right? It&amp;rsquo;s Database as a Service where you can just spin up but Riak is a little bit different. It&amp;rsquo;s not hosted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: I&amp;rsquo;m not entirely sold on what DynamoDB is for because my general feeling is if you require such extreme amount of up-time where you need to use something like Riak. Just completely handing that off to a third-party seems weird to me. Just giving someone else control over one thing, in exchange you get a lot of control over it. I think you can set your own R, W, N values but who cares; I mean you don&amp;rsquo;t own these servers anyways. If I were going to use a host solution because I didn&amp;rsquo;t care that much, I would just use a database that&amp;rsquo;s easier to use. There&amp;rsquo;s MongoHQ and &lt;a href="https://mongolab.com/welcome/"&gt;MongoLab&lt;/a&gt; if you want to do that; There&amp;rsquo;s &lt;a href="http://redistogo.com/"&gt;Redis To Go&lt;/a&gt; if you want to use hosted Redis; There&amp;rsquo;s a &lt;a href="https://www.heroku.com/postgres"&gt;Horoku Postgres&lt;/a&gt;. There&amp;rsquo;s a lot of other options for hosted databases. I&amp;rsquo;ll probably going to get flagged for saying that but I&amp;rsquo;m not telling you that you don&amp;rsquo;t use it. If you get a good use case, go ahead and use it, but it definitely wouldn&amp;rsquo;t be my first choice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: One scenario of getting this kind of infrastructure is if your infrastructure is solely based on EC2 then that&amp;rsquo;s great; if you have infrastructure away not on Amazon&amp;rsquo;s cloud then you&amp;rsquo;ll have to pay extra latency to retrieve the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: That&amp;rsquo;s true. Of course they designed it that way. They absolutely want you to use all of their infrastructure for everything and they&amp;rsquo;ve very much set it up in that way. There are outline cases: if you have a lot of money and you&amp;rsquo;re a huge customer you can use &lt;a href="https://aws.amazon.com/directconnect/"&gt;Amazon Direct Connect&lt;/a&gt; which costs like six grand a month or something and they&amp;rsquo;ll co-locate a fat pipe for relatively low latency. You can do all of these things but if you&amp;rsquo;re to that level, why are you using EC2 anyway? Why are you letting them host anything? There&amp;rsquo;s a lot of customers that love it and I can&amp;rsquo;t speak too poorly because people are using it successfully; Maybe I&amp;rsquo;m a little more paranoid than everyone else. We haven&amp;rsquo;t covered Neo4j at all.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: It&amp;rsquo;s coming up. It&amp;rsquo;s the next one. But I want to ask the last question for Riak because I think you&amp;rsquo;re more familiar with that. Other than e-commerce sites such as Amazon, what are other use cases or scenarios that can benefit from high availability?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: There are all sorts: we have video games companies that are using our stuff to be the back-end for user data and session data, etc or even switching devices where for example on a lot of games you can play on multiple devices: you can save state in one and pick up another and continue playing. These kinds of things are at a very large scale. (They) seem trivial but they&amp;rsquo;re very hard to get right and you can&amp;rsquo;t really have a lot of latency in those cases. So it goes beyond just simple shopping carts. There&amp;rsquo;s also Riak CS which is great for asset storage. People store videos and images. As long as your values are small, you can store anything in Riak. Like I said, values are opaque so a value can be an image, just basics before encoded in whatever. It could just be small thumbnails if you&amp;rsquo;d like although I would recommend (Riak) CS but you could even use regular Riak as long as your values are bounded. But if you&amp;rsquo;re unbounded or objects, CS is a good option.&lt;/p&gt;

&lt;h4&gt;Part 6: Graph Databases&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Cool. The last one is a graph database and it&amp;rsquo;s Neo4j.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: I love Neo4j.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Isn&amp;rsquo;t it just social networks? What else?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;:  It&amp;rsquo;s not. That&amp;rsquo;s the interesting thing. It&amp;rsquo;s much more like a relational database in the sense that the relationships between values are as important as the values themselves. This isn&amp;rsquo;t really true for other NoSQL databases largely because you kind of denormalize them by design. We&amp;rsquo;ve talked about document data stores and key-value store and even column-oriented stores but when you have a graph data store, you can get the best of the worlds in the sense that you have the flexibility of normalizing or denormalizing as much as you want and you can then join values effectively by graph walking. What I mean by this is you start at a node and then your query is okay from this node find all the nodes that match these criteria. The criteria might be find all nodes that are adjacent to this node. That&amp;rsquo;s the simplest case. And that would be like a social network, say just find all friends. But it can be find all friends of friends: so walk two nodes out. And you can filter as part of these queries as well: find all friends of friends that graduated from this university assuming that data is in the graph somewhere, find all friends of friends that went to this university that themselves have a friend&amp;rsquo;s named Jeff. You can just keep building these ridiculously complex queries with something that you didn&amp;rsquo;t necessarily think of in advance. I&amp;rsquo;m using social network as an example because it&amp;rsquo;s the easiest case for people to wrap their heads around but there are a lot of things that you can use graphs for. One of my favourite ones, because I&amp;rsquo;m such a nerd about language and words, is you can have a graph-based thesaurus and your graph can be weighted as well. How close is this other word to this according to some weight? And you can look up some word like little and say what&amp;rsquo;s a similar word to little. Well, small is similar to little. You can also have antonyms connected and say okay big is connected to little but its edge will be labelled as an antonym, so if you&amp;rsquo;re looking for antonyms just do one step out to find adjacent antonyms. It&amp;rsquo;s a very quick look-up because once you found that first node, which you can just index. It&amp;rsquo;s a straight-forward look-up: find the word little and then find all the adjacent words to this. Now imagine doing this in a relational database. It would actually be kind of complex because you would have to replicate a graph in a relational style. You would have some table called words and you might have a join table that links a word to another word in some way, whether it&amp;rsquo;s a synonym or an antonym and now the way to find those synonyms or antonyms is to do a join from the word table to the join table, back to the word table. So it&amp;rsquo;s like a self-referential join. It&amp;rsquo;d actually become more complex to query it in a relational way than it&amp;rsquo;s in a graph data store version where we can just say I start at here, walk one node out, give me what you got. But you can also do really interesting things such as sorting by weight, which you could do in a relational way as well but now you&amp;rsquo;ve got to hang something off that join table. This is why one we created and joined the table in the first place so that we can have these weights and statuses of antonyms and synonyms. The sort of argument for Neo4j (this is something I&amp;rsquo;ve said in the book and something I&amp;rsquo;ve said in many talks before) is it&amp;rsquo;s a whiteboard friendly data structure. If you can draw on a whiteboard, you can just model it directly in that way and you&amp;rsquo;re done. The other thing is, you can add things that wouldn&amp;rsquo;t have necessarily thought of in advance. So besides having the query flexibility of something like a relational data store, you also have the storage flexibility of something like Mongo, Couch or Riak where you could say, I didn&amp;rsquo;t originally envision that our giant thesaurus graph: we might also decide to create a subgraph of books that contain those words and the link might be a count of how many times that word appears. So again, really simple look-up. I have the word best. What books contain the word best? I&amp;rsquo;m sure there&amp;rsquo;ll be a lot. Best of times, worst of times. It&amp;rsquo;ll be a lot of books but it would again be a very quick look-up. It would just say okay one step find all these adjacent values. It&amp;rsquo;s something you wouldn&amp;rsquo;t have necessarily designed in your system upfront but it&amp;rsquo;s something that you can add later. And the final thing that I think it&amp;rsquo;s really interesting is just the graph algorithms that you can do. In my book, the example I give is the &lt;a href="http://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon"&gt;Six Degrees of Kevin Bacon&lt;/a&gt; example where I uploaded a bunch of movies and actors and say okay, walk this graph and find the shortest path between this actor and Kevin Bacon through movies that they mutually acted in. What was interesting when I uploaded this data set is I found the answers Neo4j gave me, despite having tens of thousands of movies and actors in it and the result would come almost instantaneously. The results are actually better than the same results in Wikipedia. For some reasons, Wikipedia has these values in it and don&amp;rsquo;t ask me why. So yeah, it&amp;rsquo;s interesting. Now there&amp;rsquo;s a downside and that being that you run into the same weakness that you have with a relational database in the fact that, in a general sense, node walking between different servers is difficult: so sharding a graph in the general sense is non-trivial thing, if it&amp;rsquo;s even possible at all because you can always make a supernode and you&amp;rsquo;ll just take your system down because every node has to walk through that supernode for non-trivial queries. I know that the Neo Technology guys are working on this, but it&amp;rsquo;s a very, very infant stage in my opinion. But it doesn&amp;rsquo;t make it less interesting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: This has came to me and correct me if I&amp;rsquo;m not on the right track. Neo4j sounds more about data modeling. Its advantage is the way it models data. Is it possible to combine Riak and Neo4j?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;:  There have been attempts. Believe it or not, you&amp;rsquo;re not the first person to ask me this, but I&amp;rsquo;m not sure where the value would lie. They&amp;rsquo;re so fundamentally different. There&amp;rsquo;s better ways of searching something like Riak than trying to model it as a graph. One of the projects that &lt;a href="https://twitter.com/rzezeski"&gt;Ryan Zezeski&lt;/a&gt; and I, for what it&amp;rsquo;s worth, have been working on is called &lt;a href="http://www.youtube.com/watch?v=ETJqu5SmwOc"&gt;Yokozuna&lt;/a&gt; and it actually uses distributed Solr as a search back-end for Riak. It&amp;rsquo;s interesting because you can do distributed search on value and it&amp;rsquo;s very, very fast and you can distribute to dozens of nodes (not sure what the largest test has been so far and it&amp;rsquo;s still on alpha). It&amp;rsquo;s definitely interesting because you can query on data in a much more versatile way and simple key-value look-ups or even secondary indexing. The only reason I bring it up is because there&amp;rsquo;s just different ways of tackling the query problems of a key-value store.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;:  The other use case for graph database is it&amp;rsquo;s very natural for things like decision-making trees because a tree is basically a graph so a lot of the artificial intelligence side of problems would be a really good fit.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Part of the things that I like about Neo4j is that there&amp;rsquo;s a project called &lt;a href="http://jung.sourceforge.net/"&gt;JUNG&lt;/a&gt;, which is a toolkit of graph algorithms and probably the most famous and  its centrality algorithm is Google&amp;rsquo;s &lt;a href="http://en.wikipedia.org/wiki/PageRank"&gt;PageRank&lt;/a&gt;. It has these algorithms predefined in there so you just import the jar and you can just use them straight up. I think that&amp;rsquo;s pretty awesome because you can make your own tiny search engine in a very Google style way (Google doesn&amp;rsquo;t use PageRank anymore but Google style for small data sets circa 2004). I don&amp;rsquo;t want to give the impression that Neo4j can only handle small data sets. It can handle billions of nodes and billions of edges: you could model Facebook on a giant Neo4j server theoretically according to node and edge count. It goes back to scaling vertically v.s horizontally but that&amp;rsquo;s something they&amp;rsquo;re trying to resolve.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dingding&lt;/strong&gt;: Eric, you have introduced many databases. So if I start a new project, what should I choose? Do you have any suggestion on the choice?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Well, it depends. What kind of project is it and what are your consistency v.s availability requirements?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: What if I don&amp;rsquo;t have, for instance, I&amp;rsquo;m just building a web app pre-scale?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Yeah, that&amp;rsquo;s what I was going to say. If you don&amp;rsquo;t have any scalability problems and you don&amp;rsquo;t have any requirements. It&amp;rsquo;s sort of asking what should my favourite food be and giving no information about yourself whatsoever. It&amp;rsquo;s really hard to say. There&amp;rsquo;s just too many variables. I would say stick to a relational database, all things being equal. If you don&amp;rsquo;t know anything about your requirements or anything at all, there are enough toolkits and knowledge floating around out there that most of the time you&amp;rsquo;re perfectly fine with something like Postgres; Or if you want to be adventurous, then try Neo4j; If you know you&amp;rsquo;re building a system that has to be up all the time, then try Riak; If you&amp;rsquo;re building a system where you have multiple nodes that are barely connected but then they have to sync up every so often, then I would say try Couch: just install Couch on every server and let them sync up to some master node eventually; If you plan to scale to massive heights, like Facebook or Google size, then look at HBase. There&amp;rsquo;s my probably non-satisfactory answer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: What about if I want to pick a database and so I don&amp;rsquo;t have to run migrations. That&amp;rsquo;s the reason to pick Mongo? Is it good enough?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Yeah, if you don&amp;rsquo;t want to run migrations, Mongo is a good one. Mongo is actually being sold recently not so much as big data, as much for fast implementation. So yeah, that&amp;rsquo;s an option but I wouldn&amp;rsquo;t necessarily overlook Neo4j in that case either. It has a query language called &lt;a href="http://docs.neo4j.org/chunked/milestone/cypher-query-lang.html"&gt;Cypher&lt;/a&gt; that I&amp;rsquo;m re-wrting the book and if I do come out with a second edition, I&amp;rsquo;ll almost definitely change this to use Cypher because it&amp;rsquo;s a sleek, cool declarative graph walking language and it&amp;rsquo;s pretty interesting. It makes it really easy to use a graph data store without having to write code in the way that you have to with &amp;hellip; It&amp;rsquo;s declarative but it&amp;rsquo;s still effectively code.&lt;/p&gt;

&lt;h4&gt;Part 7: Picks&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: I think that&amp;rsquo;s all the questions. Let&amp;rsquo;s get to the last phase of this episode, which is picks. Picks is basically where we invite the guest to share something interesting: something you have been doing or you&amp;rsquo;ve came across with and just share with the audience.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Sure. Recently, I was accepted to be on the &lt;a href="http://www.google.com/glass/start/how-to-get-one/"&gt;Google Glass Explorer Program&lt;/a&gt; so I&amp;rsquo;m actually toying with the idea of writing a Glass book if I have the time. We&amp;rsquo;ll see. But I&amp;rsquo;ve been enjoying researching it. Originally the announcement from Google was we&amp;rsquo;re only going to have a web-based only API and it&amp;rsquo;s called the Google Mirror API but at Google I/O last week, they announced that they&amp;rsquo;re going to have a native SDK so you can run native Android app because Glass actually just runs a version of Android. I think that&amp;rsquo;s going to be pretty amazing. I&amp;rsquo;m really excited about this. Somebody has already made one of the first Android apps called Blink where you just blink your eyes and it&amp;rsquo;ll take a photo. So I&amp;rsquo;m really to excited to get to play with this and we&amp;rsquo;ll see what pans out.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: That sounds fantastic. My pick this week is, I&amp;rsquo;m actually reading a book. It&amp;rsquo;s by &lt;a href="http://en.wikipedia.org/wiki/Nassim_Taleb"&gt;Nassim Nicholas Taleb&lt;/a&gt;. He&amp;rsquo;s the author of &lt;a href="http://en.wikipedia.org/wiki/The_Black_Swan_(2007_book)"&gt;The Black Swan&lt;/a&gt;. His new book is called &lt;a href="http://www.amazon.com/Antifragile-Things-That-Gain-Disorder/dp/1400067820"&gt;Antifragile&lt;/a&gt;. If you think &lt;em&gt;The Black Swan&lt;/em&gt; is one of his big ideas then this is where he puts everything together. He says in the book that &lt;em&gt;The Black Swan&lt;/em&gt; a prequel to this book. This is where a lot of his thoughts are coming together. The big idea for this book is he claims that the opposite of fragile is not robust: the opposite of fragile is something that gains from disorder, something that gains from chaos. There&amp;rsquo;s no word in English or any other language that can express this concept. So he coined this word called antifragile to capture this idea and he then applies this concept into many domains like social economics, food, health, philosophy and of course finance (that&amp;rsquo;s his background). So it&amp;rsquo;s been pretty amazing. I feel like it&amp;rsquo;s going to be one of the door-opening or mind-opening books.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: What&amp;rsquo;s an example of something that&amp;rsquo;s antifragile?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: For instance, rumours: the more you want to suppress rumours then the more they would spread. He actually uses the example of Hydra where you cut off one head and two heads would come out. If you say something is robust, that means it would resist chaos, it would resist change so at best it would not change but antifragile is something that if chaos happens, it would become more robust. This is a concept that&amp;rsquo;s not in any language or domains so he opened this door. I think it&amp;rsquo;s a fascinating book. It&amp;rsquo;s funny because the philosophy is much like reading Tao Te Ching because the more you try to impose structure, the more you try to control things then it becomes more fragile.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: The tao that can be named is not the tao. It&amp;rsquo;s awesome.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: Yeah, so that&amp;rsquo;s my pick.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dingding&lt;/strong&gt;: Basically I have no picks so I&amp;rsquo;ll show you something interesting this week. I have watched the two videos recorded by Hashrocket with Eric and I think it&amp;rsquo;s very good. So I&amp;rsquo;ll recommend it to our audience and you&amp;rsquo;ll learn more about the background of this episode.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: One thing I should mention too is I just released a book a couple weeks ago. It&amp;rsquo;s on &lt;a href="http://littleriakbook.com/"&gt;littleriakbook.com&lt;/a&gt; and it&amp;rsquo;s just a short 60-page book that explains Riak and it&amp;rsquo;s free.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: OK, Eirc. Thank you so much for taking the time to talk with us today.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: That was a blast. Thanks.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kevin&lt;/strong&gt;: It was a great conversation. I learned so much more about database from this interview and I hope our audience will like it as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eric&lt;/strong&gt;: Thanks. I appreciate it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All&lt;/strong&gt;: Take care. Bye.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;span class="footnotes"&gt;
  再次感谢&lt;a href="http://weibo.com/wuyicun"&gt;@吴怡村&lt;/a&gt;对于本期节目音频的整理。
&lt;/span&gt;&lt;/p&gt;
</content>
  </entry>
</feed>
